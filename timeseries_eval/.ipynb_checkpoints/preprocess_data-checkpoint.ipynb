{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = 'sine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sine_data(no, seq_len, dim):   \n",
    "    \"\"\"Sine data generation.\n",
    "    Args:\n",
    "    - no: the number of samples\n",
    "    - seq_len: sequence length of the time-series\n",
    "    - dim: feature dimensions\n",
    "\n",
    "    Returns:\n",
    "    - data: generated data\n",
    "    \"\"\" \n",
    "    size = (no, 1, dim)\n",
    "    freq = np.random.uniform(0, 1, size)    \n",
    "    phase = np.random.uniform(-1, 1, size)\n",
    "    \n",
    "    seq = np.arange(seq_len)\n",
    "    seq = np.expand_dims(seq, axis=0)\n",
    "    seq = np.expand_dims(seq, axis=-1)\n",
    "    \n",
    "    data = np.sin(freq * seq + phase)    \n",
    "#     data = (data + 1) * 0.5\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, D = 10000, 24, 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PERC = 0.05\n",
    "TRAIN_PERC = 1- TEST_PERC\n",
    "\n",
    "perc_of_train_data = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_datasets = []\n",
    "for p in perc_of_train_data:\n",
    "    sine_data = gen_sine_data(int(N * p / 100), T, D)\n",
    "    sine_data = np.array(sine_data)\n",
    "    \n",
    "    sine_datasets.append(sine_data)\n",
    "    #print(sine_data.shape)\n",
    "    \n",
    "    fname = f'{selected}_subsampled_train_perc_{p}.npy'\n",
    "    full_path = os.path.join(data_dir, fname)\n",
    "    \n",
    "    np.save(full_path, sine_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMax Scale and Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom scaler for 3d data\n",
    "class TSMinMaxScaler():\n",
    "    '''Scales history and forecast parts of time-series based on history data'''\n",
    "    def __init__(self, forecast_len, input_dim, upper_bound = 5.):         \n",
    "        self.forecast_len = forecast_len\n",
    "        self.min_vals_per_d = None      \n",
    "        self.max_vals_per_d = None  \n",
    "        self.input_dim = input_dim\n",
    "        self.upper_bound = upper_bound\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        curr_len = X.shape[1]\n",
    "        self.scaling_len = curr_len - self.forecast_len\n",
    "        # print(self.scaling_len); sys.exit()\n",
    "\n",
    "        if self.scaling_len < 1: \n",
    "            msg = f''' Error scaling series. \n",
    "            scaling_len needs to be at least 2. Given length is {self.scaling_len}.  '''\n",
    "            raise Exception(msg)\n",
    "\n",
    "        self.min_vals_per_d = np.expand_dims( X[ :,  : self.scaling_len , : ].min(axis=1), axis = 1)\n",
    "        self.max_vals_per_d = np.expand_dims( X[ :,  : self.scaling_len , : ].max(axis=1), axis = 1)\n",
    "        self.range_per_d = self.max_vals_per_d - self.min_vals_per_d\n",
    "\n",
    "        self.range_per_d = np.where(self.range_per_d == 0, 1e-5, self.range_per_d)\n",
    "              \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):         \n",
    "        assert X.shape[0] == self.min_vals_per_d.shape[0], \"Error: Dimension of array to scale doesn't match fitted array.\"\n",
    "        assert X.shape[2] == self.min_vals_per_d.shape[2], \"Error: Dimension of array to scale doesn't match fitted array.\"\n",
    "         \n",
    "        X = X - self.min_vals_per_d\n",
    "        X = np.divide(X, self.range_per_d )        \n",
    "        X[:, :, :self.input_dim] = np.where( X[:, :, :self.input_dim] < self.upper_bound, X[:, :, :self.input_dim], self.upper_bound)\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "        \n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X[:, :, : self.input_dim] = X[:, :, : self.input_dim ] * self.range_per_d[:, :, : self.input_dim] \n",
    "        X[:, :, : self.input_dim] = X[:, :, : self.input_dim] + self.min_vals_per_d[:, :, : self.input_dim]\n",
    "        # print(X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled_datasets = []\n",
    "for p, d in zip(perc_of_train_data, sine_datasets):\n",
    "    scaler = TSMinMaxScaler(\n",
    "        forecast_len = 20,\n",
    "        input_dim = D,\n",
    "        upper_bound = 5.0\n",
    "    )    \n",
    "    \n",
    "    scaled_data = scaler.fit_transform(d)\n",
    "    np.save(os.path.join(data_dir, f'{selected}_scaled_train_perc_{p}.npy'), scaled_data)\n",
    "    joblib.dump(scaler, os.path.join(data_dir, f'{selected}_scaler_perc_{p}.save'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy and Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'energy': {\n",
    "        'file': 'Energy-energydata_complete.csv',\n",
    "        'time_col': 'date',\n",
    "    },\n",
    "    'stocks': {\n",
    "        'file': 'stock_data.csv',\n",
    "        'time_col': None, \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected = 'energy'\n",
    "selected = 'stocks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Open       High        Low      Close  Adj_Close    Volume\n",
      "0  49.676899  51.693783  47.669952  49.845802  49.845802  44994500\n",
      "1  50.178635  54.187561  49.925285  53.805050  53.805050  23005800\n",
      "2  55.017166  56.373344  54.172661  54.346527  54.346527  18393200\n",
      "3  55.260582  55.439419  51.450363  52.096165  52.096165  15361800\n",
      "4  52.140873  53.651051  51.604362  52.657513  52.657513   9257400\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(data_dir, data_dict[selected]['file'])\n",
    "\n",
    "if data_dict[selected]['time_col'] is not None: \n",
    "    data = pd.read_csv(file_path, parse_dates=[data_dict[selected]['time_col']])\n",
    "else:\n",
    "    data = pd.read_csv(file_path)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (3685, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"data shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort data by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "time_col = data_dict[selected]['time_col']\n",
    "print(time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_col in data: \n",
    "    data.sort_values(by=time_col, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the time col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (3685, 6)\n"
     ]
    }
   ],
   "source": [
    "if time_col in data: del data[time_col]\n",
    "print(\"data shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac:20%, train_data_frac shape: (700, 6)\n"
     ]
    }
   ],
   "source": [
    "N = data.shape[0]\n",
    "\n",
    "frac_datasets = []\n",
    "for p in perc_of_train_data: \n",
    "    \n",
    "    num_test = int(N * TEST_PERC)   # to be held-out\n",
    "    orig_num_train = N - num_test\n",
    "    \n",
    "    test_data = data.tail(num_test)  # to be used for evaluation later\n",
    "    \n",
    "    orig_train_data = data.loc[np.arange(orig_num_train)]\n",
    "    #print(orig_train_data.shape)\n",
    "    \n",
    "    num_train_frac = int(orig_num_train * p / 100.)\n",
    "    #print(f'Num train steps for p = {p} is {num_train_frac}')\n",
    "    \n",
    "    train_data_frac = data.tail(num_train_frac) \n",
    "    frac_datasets.append(train_data_frac)\n",
    "    print(f'frac:{p}%, train_data_frac shape: {train_data_frac.shape}' )\n",
    "    \n",
    "    #np.save(os.path.join(data_dir, f'{selected}_train_perc_{p}.npy'), train_data_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scale Data and Save Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled_frac_datasets = []\n",
    "for p, d_tup in zip(perc_of_train_data, frac_datasets):\n",
    "    \n",
    "    train_data, test_data = d_tup    \n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    train_data_arr = scaler.fit_transform(train_data)\n",
    "    test_data_arr = scaler.transform(test_data)   \n",
    "    \n",
    "    scaled_frac_datasets.append(train_data_arr)\n",
    "    \n",
    "    \n",
    "    np.save(os.path.join(data_dir, f'{selected}_scaled_test_perc_{p}.npy'), test_data_arr)\n",
    "    \n",
    "    joblib.dump(scaler, os.path.join(data_dir, f'{selected}_scaler_perc_{p}.save'))     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to 3D tensors\n",
    "shape = N, T, D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped:  (1, 700, 6)\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(frac_datasets):\n",
    "    frac_datasets[i] = d.values.reshape(1, *d.shape)\n",
    "    print(\"reshaped: \", frac_datasets[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Subsampled series of required seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 20 3d tensor shape: (676, 24, 6)\n"
     ]
    }
   ],
   "source": [
    "target_len = 24\n",
    "for p, d in zip(perc_of_train_data, frac_datasets):\n",
    "    subsampled_dataset = []\n",
    "    for idx in range(d.shape[1] - T):\n",
    "        ser = d[:, idx: idx+T, :]\n",
    "        subsampled_dataset.append(ser)\n",
    "    subsampled_dataset = np.concatenate(subsampled_dataset, axis=0)\n",
    "    print('p:', p, '3d tensor shape:', subsampled_dataset.shape)\n",
    "    \n",
    "    np.save(os.path.join(data_dir, f'{selected}_subsampled_train_perc_{p}.npy'), subsampled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
