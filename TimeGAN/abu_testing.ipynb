{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sys \n",
    "\n",
    "from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "from metrics.predictive_metrics import predictive_score_metrics\n",
    "from metrics.visualization_metrics import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data_dir = './data/orig/'\n",
    "gen_data_dir = './data/generated/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler():\n",
    "    \"\"\"Min Max normalizer.\n",
    "    Args:\n",
    "    - data: original data\n",
    "\n",
    "    Returns:\n",
    "    - norm_data: normalized data\n",
    "    \"\"\"\n",
    "    def fit_transform(self, data): \n",
    "        self.fit(data)\n",
    "        scaled_data = self.transform(data)\n",
    "        return scaled_data\n",
    "\n",
    "\n",
    "    def fit(self, data):    \n",
    "        self.mini = np.min(data, 0)\n",
    "        self.range = np.max(data, 0) - self.mini\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def transform(self, data):\n",
    "        numerator = data - self.mini\n",
    "        scaled_data = numerator / (self.range + 1e-7)\n",
    "        return scaled_data\n",
    "\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        data *= self.range\n",
    "        data += self.mini\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric_iteration = 3\n",
    "\n",
    "# full selection of data to run\n",
    "training_sizes = [2, 5, 10, 20, 100]\n",
    "datasets = ['sine', 'stocks', 'air', 'energy']\n",
    "\n",
    "\n",
    "# custom selection \n",
    "training_sizes = [ 100 ]\n",
    "# datasets = ['sine']\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    for training_size in training_sizes:\n",
    "\n",
    "        print('-'*90); print('-'*90)\n",
    "        print(f\"Data: {dataset}; Training Size: {training_size}\")\n",
    "\n",
    "    #     original data\n",
    "        fname = f'{orig_data_dir + dataset}_subsampled_train_perc_{training_size}.npz'\n",
    "        loaded = np.load(fname)\n",
    "        ori_data = loaded['data']\n",
    "        print('orig data shape: ', ori_data.shape)\n",
    "\n",
    "        # generated data\n",
    "        sample_file_name = gen_data_dir + f'vae_conv_I_gen_samples_{dataset}_perc_{training_size}.npz'\n",
    "        loaded = np.load(sample_file_name)\n",
    "        gen_data = loaded['data']\n",
    "\n",
    "#         print('orig means: ', ori_data.mean(axis=(0, 2)))\n",
    "#         print('gen means: ', gen_data.mean(axis=(0, 2)))\n",
    "\n",
    "        scaler_orig = MinMaxScaler( )  \n",
    "        scaled_ori_data = scaler_orig.fit_transform(ori_data)\n",
    "        scaled_gen_data = scaler_orig.transform(gen_data)    \n",
    "    #     print('generated_data shape:', generated_data.shape)\n",
    "    #     ---------------------------------------------------------------------------\n",
    "        print(\"-\"*90); print('Visualizations:')\n",
    "    #     visualization(ori_data[0:generated_data.shape[0]], generated_data, 'pca')\n",
    "        visualization(ori_data[0:generated_data.shape[0]], generated_data, 'tsne')\n",
    "\n",
    "        #     ---------------------------------------------------------------------------\n",
    "        print(\"-\"*90); print('Discrimination Score :')\n",
    "        discriminative_score = list()\n",
    "        for iter in range(metric_iteration):\n",
    "            temp_disc = discriminative_score_metrics(ori_data, generated_data)\n",
    "            discriminative_score.append(temp_disc)  \n",
    "            print(\"----------  disc iter: \", iter, 'score: ', temp_disc, '----------')\n",
    "\n",
    "        print(\"-\"*90); print('Discrimination Score :')\n",
    "        print('Discriminative score: ' + str(np.round(np.mean(discriminative_score), 4)))\n",
    "        print(\"Discriminative score CI: \", confidence_interval(discriminative_score)[1])\n",
    "\n",
    "        #     ---------------------------------------------------------------------------             \n",
    "        print(\"-\"*90); print('Predictive Score :')\n",
    "        predictive_score = list()\n",
    "        for tt in range(metric_iteration):\n",
    "            temp_pred = predictive_score_metrics(ori_data, generated_data, iterations = 4000)\n",
    "            predictive_score.append(temp_pred)   \n",
    "            print(\"----------  pred iter: \", iter, 'score: ', temp_pred, '----------')\n",
    "        print('Predictive score: ' + str(np.round(np.mean(predictive_score), 4)))\n",
    "        print(\"Predictive score CI: \", confidence_interval(predictive_score)[1])\n",
    "\n",
    "        print(\"\\n\")\n",
    "        #     ---------------------------------------------------------------------------\n",
    "    #     visualization(ori_data[0:generated_data.shape[0]], generated_data, 'pca', \"sine_pca_\" + str(int(training_size)))\n",
    "    #     visualization(ori_data[0:generated_data.shape[0]], generated_data, 'tsne', \"sine_tsne_\" +str(int(training_size)))\n",
    "        \n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 1.15 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/tensorflow-1.15-cpu-py37-ubuntu18.04-v7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
