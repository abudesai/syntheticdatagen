{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/site-packages (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/site-packages (from scipy) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sys \n",
    "import time\n",
    "\n",
    "# from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "# from metrics.predictive_metrics import predictive_score_metrics\n",
    "\n",
    "from metrics.discriminative_metrics2 import discriminative_score_metrics\n",
    "from metrics.predictive_metrics2 import predictive_score_metrics\n",
    "from metrics.visualization_metrics import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data_dir = \"../data/processed_orig_data/\"\n",
    "gen_data_dir = \"../data/generated_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler():\n",
    "    \"\"\"Min Max normalizer.\n",
    "    Args:\n",
    "    - data: original data\n",
    "\n",
    "    Returns:\n",
    "    - norm_data: normalized data\n",
    "    \"\"\"\n",
    "    def fit_transform(self, data): \n",
    "        self.fit(data)\n",
    "        scaled_data = self.transform(data)\n",
    "        return scaled_data\n",
    "\n",
    "\n",
    "    def fit(self, data):    \n",
    "        self.mini = np.min(data, 0)\n",
    "        self.range = np.max(data, 0) - self.mini\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def transform(self, data):\n",
    "        numerator = data - self.mini\n",
    "        scaled_data = numerator / (self.range + 1e-7)\n",
    "        return scaled_data\n",
    "\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        data *= self.range\n",
    "        data += self.mini\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Data: stocks; Training Size: 100\n",
      "orig data shape:  (3661, 24, 6)\n",
      "------------------------------------------------------------------------------------------\n",
      "Predictive Score :\n",
      "Avg. train loss for epoch 0: 0.119 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "metric_iteration = 1\n",
    "\n",
    "# full selection of data to run\n",
    "training_sizes = [2, 5, 10, 20, 100]\n",
    "datasets = ['sine', 'stocks', 'air', 'energy']\n",
    "\n",
    "\n",
    "# custom selection \n",
    "training_sizes = [ 100 ]\n",
    "datasets = ['stocks']\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    for training_size in training_sizes:\n",
    "\n",
    "        print('-'*90); print('-'*90)\n",
    "        print(f\"Data: {dataset}; Training Size: {training_size}\")\n",
    "\n",
    "    #     original data\n",
    "        fname = f'{orig_data_dir + dataset}_subsampled_train_perc_{training_size}.npz'\n",
    "        loaded = np.load(fname)\n",
    "        ori_data = loaded['data']\n",
    "        print('orig data shape: ', ori_data.shape)\n",
    "\n",
    "        # generated data\n",
    "        sample_file_name = gen_data_dir + f'vae_conv_I_gen_samples_{dataset}_perc_{training_size}.npz'\n",
    "        loaded = np.load(sample_file_name)\n",
    "        gen_data = loaded['data']\n",
    "\n",
    "#         print('orig means: ', ori_data.mean(axis=(0, 2)))\n",
    "#         print('gen means: ', gen_data.mean(axis=(0, 2)))\n",
    "\n",
    "        scaler_orig = MinMaxScaler( )  \n",
    "        scaled_ori_data = scaler_orig.fit_transform(ori_data)\n",
    "        scaled_gen_data = scaler_orig.transform(gen_data)    \n",
    "    #     print('generated_data shape:', generated_data.shape)\n",
    "    #     ---------------------------------------------------------------------------\n",
    "        print(\"-\"*90); print('Visualizations:')\n",
    "        visualization(scaled_ori_data[0:scaled_gen_data.shape[0]], scaled_gen_data, 'pca')\n",
    "        visualization(scaled_ori_data[0:scaled_gen_data.shape[0]], scaled_gen_data, 'tsne')\n",
    "\n",
    "            ---------------------------------------------------------------------------\n",
    "        print(\"-\"*90); print('Discrimination Score :')\n",
    "        discriminative_score = list()\n",
    "        for tt in range(metric_iteration):\n",
    "            temp_disc = discriminative_score_metrics(scaled_ori_data, scaled_gen_data,  epochs = 1000)\n",
    "            discriminative_score.append(temp_disc)  \n",
    "            print(\"----------  disc iter: \", tt, 'score: ', temp_disc, '----------')\n",
    "\n",
    "        print(\"-\"*90); print('Discrimination Score :')\n",
    "        print('Discriminative score: ' + str(np.round(np.mean(discriminative_score), 4)))\n",
    "        print(\"Discriminative score CI: \", confidence_interval(discriminative_score)[1])\n",
    "\n",
    "        #     ---------------------------------------------------------------------------             \n",
    "        print(\"-\"*90); print('Predictive Score :')\n",
    "        predictive_score = list()\n",
    "        for tt in range(metric_iteration):\n",
    "            temp_pred = predictive_score_metrics(scaled_ori_data, scaled_gen_data, epochs = 500)\n",
    "            predictive_score.append(temp_pred)   \n",
    "            print(\"----------  pred iter: \", tt, 'score: ', temp_pred, '----------')\n",
    "        print('Predictive score: ' + str(np.round(np.mean(predictive_score), 4)))\n",
    "        print(\"Predictive score CI: \", confidence_interval(predictive_score)[1])\n",
    "\n",
    "        print(\"\\n\")\n",
    "        #     ---------------------------------------------------------------------------\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total run time: {np.round((end - start)/60.0, 2)} minutes\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.1 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/tensorflow-2.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
