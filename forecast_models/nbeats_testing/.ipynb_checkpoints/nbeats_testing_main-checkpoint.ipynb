{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,Callback,LearningRateScheduler, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nbeats_model import NBeatsNet as NBeatsNet\n",
    "warnings.filterwarnings(action='ignore', message='Setting attributes')\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "\n",
    "from utils import TSMinMaxScaler, DailyAggregator, generate_sine_data, MinMaxScaler_Feat_Dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: 913 24 28\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/'\n",
    "dataset = 'energy'            # sine, stocks, energy\n",
    "perc = 5                    # 5, 10, 20, 100\n",
    "train_on = 'generated_data'         # 'real_data', 'generated_data'\n",
    "vae_type = 'convI'\n",
    "\n",
    "input_file = f'{dataset}_subsampled_train_perc_{perc}.npy'\n",
    "\n",
    "data = np.load(data_dir + input_file)\n",
    "\n",
    "N, T, D = data.shape   \n",
    "print('data shape:', N, T, D) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "backcast_length=20\n",
    "forecast_length=4\n",
    "seq_len = backcast_length + forecast_length\n",
    "\n",
    "input_dim = output_dim = D\n",
    "scaler_upper_bound = 5.\n",
    "scaler_lower_bound = -5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Validation Split In Samples On True Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(821, 24, 28) (92, 24, 28)\n"
     ]
    }
   ],
   "source": [
    "valid_perc = 0.1\n",
    "\n",
    "N_train = int(N * (1 - valid_perc))\n",
    "N_valid = N - N_train\n",
    "\n",
    "# Shuffle data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "true_train_data = data[:N_train]\n",
    "true_valid_data = data[N_train:]\n",
    "\n",
    "print(true_train_data.shape, true_valid_data.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read synthetic data as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#syn_train_data = true_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/vae_conv_generated_energy_subsampled_train_perc_5.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-346-a452d7e9ce7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'vae_{vae_type}_generated_{dataset}_subsampled_train_perc_{perc}.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msyn_train_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyn_train_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/vae_conv_generated_energy_subsampled_train_perc_5.npy'"
     ]
    }
   ],
   "source": [
    "if train_on == 'real_data':\n",
    "    syn_train_data = true_train_data\n",
    "else: \n",
    "    fname = f'vae_{vae_type}_generated_{dataset}_subsampled_train_perc_{perc}.npy'\n",
    "    syn_train_data = np.load(data_dir + fname)\n",
    "print(syn_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = syn_train_data\n",
    "valid_data = true_valid_data\n",
    "\n",
    "print(train_data.shape,  valid_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add reversed series for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate([train_data, np.flip(train_data, axis=1)], axis=0)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Max Scale Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_scaler = TSMinMaxScaler(\n",
    "    scaling_len = backcast_length,\n",
    "    input_dim = input_dim,\n",
    "    upper_bound = scaler_upper_bound,\n",
    "    lower_bound = scaler_lower_bound\n",
    ")\n",
    "scaled_train_data = train_data_scaler.fit_transform(train_data)\n",
    "\n",
    "\n",
    "valid_data_scaler = TSMinMaxScaler(\n",
    "    scaling_len = backcast_length,\n",
    "    input_dim = input_dim,\n",
    "    upper_bound = scaler_upper_bound,\n",
    "    lower_bound = scaler_lower_bound\n",
    ")\n",
    "scaled_valid_data = valid_data_scaler.fit_transform(valid_data)\n",
    "\n",
    "print(scaled_train_data.shape, scaled_valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_data.mean(), scaled_valid_data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X / Y Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = scaled_train_data[:, :backcast_length], scaled_train_data[:, backcast_length:]\n",
    "x_valid, y_valid = scaled_valid_data[:, :backcast_length], scaled_valid_data[:, backcast_length:]\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.mean(), y_train.mean(), x_valid.mean(), y_valid.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() \n",
    "\n",
    "models = []\n",
    "for d in range(D):\n",
    "    \n",
    "    print(f'-----------dim: {d}----------------')\n",
    "    \n",
    "    X, Y = x_train[:, :, d: d +1].copy(), y_train[:, :, d: d +1].copy()\n",
    "    # X, Y = x_valid[:, :, d: d +1].copy(), y_valid[:, :, d: d +1].copy()\n",
    "\n",
    "    model = NBeatsNet(\n",
    "            input_dim=1,\n",
    "            backcast_length=backcast_length, \n",
    "            forecast_length=forecast_length,\n",
    "            stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n",
    "            nb_blocks_per_stack=3, \n",
    "            thetas_dim=(36, 36), \n",
    "            share_weights_in_stack=False,\n",
    "            hidden_layer_units=50\n",
    "        )\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    \n",
    "    logpath = \"./log/\"\n",
    "    logname = f'Log_nBeats_{dataset}_dim_{d}.txt'\n",
    "    logfile = os.path.join(logpath, logname)\n",
    "    csv_logger = CSVLogger(logfile, append=True)\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "    early = EarlyStopping(monitor='val_loss', mode='min', patience=10) # prevent overfitting\n",
    "\n",
    "    mdpath = \"./model/\"\n",
    "    mdname = f'nBeats_{dataset}_dim_{d}.h5'\n",
    "    mdfile = os.path.join(mdpath, mdname)\n",
    "\n",
    "    ckpt = ModelCheckpoint(mdfile, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #save whole model\n",
    "    callbacks_list = [ckpt, early, csv_logger, reduceLR]\n",
    "    callbacks_list = [ckpt, early, csv_logger, reduceLR]\n",
    "    \n",
    "\n",
    "    # Train the model.\n",
    "    print('Training...')\n",
    "\n",
    "    model.fit(X, \n",
    "              Y, \n",
    "              validation_split=0.2, \n",
    "              batch_size=64, \n",
    "              shuffle=True, \n",
    "              epochs=20, \n",
    "              callbacks=callbacks_list,\n",
    "              verbose = 0\n",
    "             )\n",
    "    \n",
    "    models.append(model )\n",
    "\n",
    "end = time.time()\n",
    "train_time = np.round((end - start)/60.0, 2)\n",
    "print(f\"Total training time: {train_time} minutes\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for d in range(D):\n",
    "    X, Y = x_valid[:, :, d: d +1].copy(), y_valid[:, :, d: d +1].copy()\n",
    "\n",
    "    # Predict on the testing set (forecast).\n",
    "    Yhat = models[d].predict(X)\n",
    "    preds.append(Yhat)\n",
    "\n",
    "\n",
    "Yhat = np.concatenate(preds, axis=-1)\n",
    "print(\"the prediction_forecast shape is:\", Yhat.shape) #shape: (30, 5, 1)\n",
    "\n",
    "X = x_valid\n",
    "Y = y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.mean(), Yhat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(Y.flatten(), Yhat.flatten())\n",
    "mae = mean_absolute_error(Y.flatten(), Yhat.flatten())\n",
    "r_sq = r2_score(Y.flatten(), Yhat.flatten())\n",
    "\n",
    "print('mse', mse)\n",
    "print('mae', mae)\n",
    "print('r_sq', r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = f'vae_{vae_type}' if train_on == f'generated_data' else 'None'\n",
    "\n",
    "perf_metrics = [ [\n",
    "            dataset,\n",
    "            generator,\n",
    "            perc,\n",
    "            'nbeats', \n",
    "            train_on,\n",
    "            np.round(mse, 3), \n",
    "            np.round(np.sqrt(mse), 3),\n",
    "            np.round(mae, 3),\n",
    "            np.round(r_sq, 3),   \n",
    "            datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"),\n",
    "            np.round(train_time, 3),\n",
    "             ]\n",
    "]\n",
    "\n",
    "columns=[\n",
    "    'Dataset_Name',\n",
    "    'Generator_Type',\n",
    "    'Perc_of_train_hist_len',\n",
    "    'Forecast_algo',\n",
    "    'Trained_on',\n",
    "    'MSE',\n",
    "    'RMSE',\n",
    "    'MAE',\n",
    "    'R-squared',\n",
    "    'Time_Ran',\n",
    "    'Train_time_minutes',\n",
    "]\n",
    "# convert to dataframe\n",
    "perf_metrics = pd.DataFrame(perf_metrics, columns=columns)  \n",
    "\n",
    "f_name = f'Results_{dataset}_traindata_perc{perc}_forecaster_nbeats_gen_trained_on_{generator}.csv'\n",
    "perf_metrics.to_csv(f\"./outputs/{f_name}\", index=False)\n",
    "\n",
    "\n",
    "perf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot utils.\n",
    "def plot_scatter(*args, **kwargs):\n",
    "    plt.plot(*args, **kwargs)\n",
    "    plt.scatter(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplots = [221, 222, 223, 224]\n",
    "plt.figure(1, figsize=(16,8))\n",
    "norm_constant = 1\n",
    "dim = 0\n",
    "\n",
    "temp_y = Y.copy()\n",
    "temp_yhat = Yhat.copy()\n",
    "temp_x = X.copy()\n",
    "\n",
    "for plot_id, i in enumerate(np.random.choice(range(len(X)), size=4, replace=False)):\n",
    "    p1 = np.expand_dims(Yhat[i][:,dim], axis=-1)\n",
    "    x1 = np.expand_dims(X[i][:,dim], axis=-1)\n",
    "    y1 = np.expand_dims(Y[i][:,dim], axis=-1)\n",
    "    ff, xx, yy = p1 * norm_constant, x1 * norm_constant, y1 * norm_constant\n",
    "    plt.subplot(subplots[plot_id])\n",
    "    plt.grid()\n",
    "    plot_scatter(range(0, backcast_length), xx, color='b')\n",
    "    plot_scatter(range(backcast_length, backcast_length + forecast_length), yy, color='g')\n",
    "    plot_scatter(range(backcast_length, backcast_length + forecast_length), ff, color='r')\n",
    "plt.savefig(\"nbeats-predictions-sines.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
