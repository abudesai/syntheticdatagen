{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import sys, os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np , pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from vae_dense_model import VariationalAutoencoderDense as VAE_Dense\n",
    "from vae_conv_model import VariationalAutoencoderConv as VAE_Conv\n",
    "from vae_conv_I_model import VariationalAutoencoderConvInterpretable as VAE_ConvI\n",
    "from config import config as cfg\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../../data/processed_orig_data/\"\n",
    "output_dir = \"../../data/generated_data/\"\n",
    "model_dir = './model/'\n",
    "log_dir = './log/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seeds(seed_value):   \n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_split(data, valid_perc):\n",
    "    N = data.shape[0]\n",
    "    N_train = int(N * (1 - valid_perc))\n",
    "    N_valid = N - N_train\n",
    "\n",
    "    # shuffle data, just in case\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # train, valid split \n",
    "    train_data = data[:N_train]\n",
    "    valid_data = data[N_train:]\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_train_valid_data(train_data, valid_data): \n",
    "    \n",
    "    _, T, D = train_data.shape\n",
    "    \n",
    "    scaler = utils.MinMaxScaler_Feat_Dim( scaling_len = T, input_dim = D, upper_bound = 3.0, lower_bound = -3.0 )        \n",
    "    scaled_train_data = scaler.fit_transform(train_data)\n",
    "    scaled_valid_data = scaler.transform(valid_data)\n",
    "    \n",
    "    return scaled_train_data, scaled_valid_data, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main VAE Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(scaled_train_data, latent_dim, reconstruction_wt, epochs = 100):\n",
    "    \n",
    "    _, T, D = scaled_train_data.shape\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Instantiate the VAE\n",
    "    vae = VAE_ConvI( seq_len=T,  feat_dim = D, latent_dim = latent_dim, hidden_layer_sizes=[50, 100, 200], \n",
    "            reconstruction_wt=reconstruction_wt,\n",
    "            # trend_poly=1, \n",
    "            # num_gen_seas=1,\n",
    "            # custom_seas = [ (7, 1)] ,     # list of tuples of (num_of_seasons, len_per_season)\n",
    "            use_residual_conn = True\n",
    "        )\n",
    "\n",
    "    vae.compile(optimizer=Adam())\n",
    "    # vae.summary() ; sys.exit()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Train the VAE\n",
    "    early_stop_loss = 'loss'\n",
    "    early_stop_callback = EarlyStopping(monitor=early_stop_loss, min_delta = 1e-1, patience=50) \n",
    "    reduceLR = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10)\n",
    "\n",
    "    history = vae.fit(\n",
    "        scaled_train_data, \n",
    "        batch_size = 32,\n",
    "        epochs=epochs,\n",
    "        shuffle = True,\n",
    "        callbacks=[early_stop_callback, reduceLR],\n",
    "        verbose = 0\n",
    "    )\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    return vae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times to run each scenario\n",
    "num_iters = 1\n",
    "\n",
    "# set 10% off for validation during VAE model development; then set to 0 for final data generation\n",
    "valid_perc = 0.0\n",
    "\n",
    "# our model name\n",
    "model = 'vae_conv_I'\n",
    "\n",
    "dataset_names = ['sine', 'stocks', 'air', 'energy']\n",
    "# percs = [2, 5, 10, 20, 100]\n",
    "\n",
    "dataset_names = [ 'sine',  'stocks', 'air', 'energy']\n",
    "percs = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running dataset = sine, perc = 100, iter = 0...\n",
      "        model  data  perc  iter  latent_dim     loss  reconst_loss  kl_loss  \\\n",
      "0  vae_conv_I  sine   100     0           8  675.016        44.525  452.391   \n",
      "\n",
      "   train_time_in_min  \n",
      "0              20.93  \n",
      "Currently running dataset = stocks, perc = 100, iter = 0...\n",
      "        model    data  perc  iter  latent_dim    loss  reconst_loss  kl_loss  \\\n",
      "0  vae_conv_I  stocks   100     0           8  98.361          7.31   61.813   \n",
      "\n",
      "   train_time_in_min  \n",
      "0               4.06  \n",
      "Currently running dataset = air, perc = 100, iter = 0...\n"
     ]
    }
   ],
   "source": [
    "# set random gen seed for reproducibiity\n",
    "set_seeds(42)\n",
    "\n",
    "main_start_time = time.time()    \n",
    "\n",
    "for data_name in dataset_names:    \n",
    "    for p in percs:  \n",
    "        \n",
    "        # file name to load\n",
    "        fname = f'{input_dir + data_name}_subsampled_train_perc_{p}.npz'\n",
    "        \n",
    "        # read data        \n",
    "        loaded = np.load(fname)\n",
    "        data = loaded['data']       \n",
    "        N, T, D = data.shape     \n",
    "#         print(fname, data.shape)      \n",
    "        \n",
    "        # perform train/valid split\n",
    "        train_data, valid_data = get_train_valid_split(data, valid_perc)        \n",
    "#         print(\"train/valid shapes: \", train_data.shape, valid_data.shape)       \n",
    "#         print(\"train/valid means: \", train_data.mean(), valid_data.mean()) \n",
    "        \n",
    "        \n",
    "        # scale data \n",
    "        scaled_train_data, scaled_valid_data, scaler = scale_train_valid_data(train_data, valid_data)      \n",
    "        #print(\"train/valid shapes: \", scaled_train_data.shape, scaled_valid_data.shape)     \n",
    "#         print(\"train/valid means : \", scaled_train_data.mean(), scaled_valid_data.mean()) \n",
    "        \n",
    "        training_times = []\n",
    "        for iter in range(num_iters):\n",
    "            print(f\"Currently running dataset = {data_name}, perc = {p}, iter = {iter}...\")\n",
    "            \n",
    "            # start timer\n",
    "            start = time.time()              \n",
    "            \n",
    "            # important hyper-parameter!!!\n",
    "            latent_dim = 8\n",
    "            \n",
    "            vae, history = train_model(scaled_train_data, latent_dim, reconstruction_wt = 5.0, epochs = 500)            \n",
    "#             print(history.history.keys())\n",
    "            \n",
    "            # stop timer and log training time \n",
    "            end = time.time()\n",
    "            train_time = np.round((end - start)/60.0, 2)\n",
    "            \n",
    "            training_times.append({\n",
    "                'model': model, 'data': data_name,  'perc': p, 'iter': iter, \n",
    "                'latent_dim': latent_dim,\n",
    "                'loss': np.round(history.history['loss'][-1], 3), \n",
    "                'reconst_loss': np.round(history.history['reconstruction_loss'][-1],3), \n",
    "                'kl_loss': np.round(history.history['kl_loss'][-1], 3), \n",
    "                'train_time_in_min': train_time,                \n",
    "            })            \n",
    "            # ----------------------------------------------------------------------------------------------\n",
    "            # Save the model \n",
    "            model_name_pref = f'{model}_{data_name}_perc_{p}_iter_{iter}_'\n",
    "            vae.save(model_dir, model_name_pref)  \n",
    "            \n",
    "        # ----------------------------------------------------------------------------------------------        \n",
    "        # Generate samples   \n",
    "        # We will save samples from the last iteration \n",
    "        samples = vae.get_prior_samples(num_samples=train_data.shape[0])\n",
    "        \n",
    "        # inverse transform using scaler \n",
    "        samples = scaler.inverse_transform(samples)        \n",
    "        \n",
    "        # save to output dir\n",
    "        samples_fpath = f'{model}_gen_samples_{data_name}_perc_{p}.npz'        \n",
    "        np.savez_compressed(os.path.join( output_dir, samples_fpath), data=samples)\n",
    "        # ----------------------------------------------------------------------------------------------        \n",
    "        # log training times for the iterations\n",
    "        log_df = pd.DataFrame.from_dict(training_times)\n",
    "        print(log_df)\n",
    "        log_file = f'{model}_{data_name}_perc_{p}_train_log.csv'\n",
    "        log_df.to_csv(log_dir + log_file, index=False)\n",
    "            \n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = np.round((end - main_start_time)/60.0, 2)\n",
    "print(f\"All done in {elapsed_time} minutes!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load generated data for Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset_names[-1]\n",
    "test_perc = percs[-1]\n",
    "\n",
    "sample_file_name = output_dir + f'vae_conv_I_gen_samples_{test_data}_perc_{test_perc}.npz'\n",
    "loaded = np.load(sample_file_name)\n",
    "gen_data = loaded['data']\n",
    "print(gen_data.shape)\n",
    "\n",
    "print(\"generated mean : \", gen_data.mean(axis=0).mean(axis=0)) \n",
    "\n",
    "utils.plot_samples(gen_data, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
