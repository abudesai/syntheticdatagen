{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import sys, os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np , pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import Optimizer # for the optimization\n",
    "from joblib import Parallel, delayed # for the parallelization\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from vae_dense_model import VariationalAutoencoderDense as VAE_Dense\n",
    "from vae_conv_model import VariationalAutoencoderConv as VAE_Conv\n",
    "from vae_conv_I_model import VariationalAutoencoderConvInterpretable as VAE_ConvI\n",
    "from config import config as cfg\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../../data/processed_orig_data/\"\n",
    "output_dir = \"../../data/generated_data/\"\n",
    "model_dir = './model/'\n",
    "log_dir = './log/'\n",
    "hpo_dir = './hpo_results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seeds(seed_value):   \n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_split(data, valid_perc):\n",
    "    N = data.shape[0]\n",
    "    N_train = int(N * (1 - valid_perc))\n",
    "    N_valid = N - N_train\n",
    "\n",
    "    # shuffle data, just in case\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # train, valid split \n",
    "    train_data = data[:N_train]\n",
    "    valid_data = data[N_train:]\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_train_valid_data(train_data, valid_data, scaling_method):         \n",
    "    if scaling_method == 'minmax':    \n",
    "        scaler = utils.MinMaxScaler( )  \n",
    "    elif scaling_method == 'standard': \n",
    "        raise NotImplementedError(f'Scaling method {scaling_method} not implemented')\n",
    "    elif scaling_method == 'yeojohnson':\n",
    "        raise NotImplementedError(f'Scaling method {scaling_method} not implemented')\n",
    "    else:         \n",
    "        raise NotImplementedError(f'Scaling method {scaling_method} not implemented')       \n",
    "          \n",
    "    scaled_train_data = scaler.fit_transform(train_data)\n",
    "    scaled_valid_data = scaler.transform(valid_data)\n",
    "    return scaled_train_data, scaled_valid_data, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main VAE Train and Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, latent_dim, n_layer1_in_20s, n_layer2_in_25s, n_layer3_in_50s, epochs = 100):\n",
    "    \n",
    "    _, T, D = train_data.shape\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Instantiate the VAE\n",
    "    vae = VAE_ConvI( seq_len=T,  \n",
    "                    feat_dim = D, \n",
    "                    latent_dim = int(latent_dim), \n",
    "                    hidden_layer_sizes=[ \n",
    "                        int(n_layer1_in_20s*20), \n",
    "                        int(n_layer2_in_25s*25),\n",
    "                        int(n_layer3_in_50s*50)], \n",
    "                # trend_poly=1, \n",
    "                # num_gen_seas=1,\n",
    "                # custom_seas = [ (7, 1)] ,     # list of tuples of (num_of_seasons, len_per_season)\n",
    "                use_residual_conn = True\n",
    "        )\n",
    "\n",
    "    vae.compile(optimizer=Adam())\n",
    "    # vae.summary() ; sys.exit()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Train the VAE\n",
    "    early_stop_loss = 'loss'\n",
    "    early_stop_callback = EarlyStopping(monitor=early_stop_loss, min_delta = 1e-1, patience=50) \n",
    "    reduceLR = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10)\n",
    "\n",
    "    history = vae.fit(\n",
    "        train_data, \n",
    "        batch_size = 32,\n",
    "        epochs=epochs,\n",
    "        shuffle = True,\n",
    "        callbacks=[early_stop_callback, reduceLR],\n",
    "        verbose = 0\n",
    "    )\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    return vae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, valid_data): \n",
    "    return model.evaluate(valid_data, verbose = 0, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO Using Scikit Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 4, 4, 'minmax']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the hyperparameter space\n",
    "param_grid = [\n",
    "    Integer(2, 10, name=\"latent_dim\"),\n",
    "    Integer(1, 5, name=\"n_layer1_in_20s\"),\n",
    "    Integer(1, 8, name=\"n_layer2_in_25s\"),\n",
    "    Integer(1, 8, name=\"n_layer3_in_50s\"),\n",
    "    Categorical(['minmax'], name=\"scaling_method\"),\n",
    "]\n",
    "\n",
    "dim_names = [\n",
    "    'latent_dim',\n",
    "    'n_layer1_in_20s',\n",
    "    'n_layer2_in_25s',\n",
    "    'n_layer3_in_50s',\n",
    "    'scaling_method',\n",
    "]\n",
    "\n",
    "default_parameters = [\n",
    "    8,              # latent_dim\n",
    "    2,             # n_layer1_in_20s \n",
    "    4,            # n_layer2_in_25s\n",
    "    4,            # n_layer3_in_50s\n",
    "    'minmax'        # scaling_method\n",
    "]\n",
    "\n",
    "default_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(param_grid)\n",
    "def objective(\n",
    "            latent_dim,\n",
    "            n_layer1_in_20s,\n",
    "            n_layer2_in_25s,\n",
    "            n_layer3_in_50s,\n",
    "            scaling_method,\n",
    "        ):\n",
    "\n",
    "    global trial_num\n",
    "    global best_loss\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # Print the hyper-parameters.   \n",
    "    print('-------------------------------------------')\n",
    "    print(f'trial_num: {trial_num}')\n",
    "    \n",
    "    print(f'latent_dim: {latent_dim}')\n",
    "    print(f'n_layer1_in_20s: {n_layer1_in_20s}')\n",
    "    print(f'n_layer2_in_25s: {n_layer2_in_25s}')\n",
    "    print(f'n_layer3_in_50s: {n_layer3_in_50s}')\n",
    "    print(f'scaling_method: {scaling_method}')   \n",
    "    print()   \n",
    "\n",
    "    trial_num += 1    \n",
    "    \n",
    "\n",
    "    losses = []\n",
    "    for train_index, valid_index in kf.split(data):  \n",
    "        \n",
    "        # grab train/test data using kf indexes\n",
    "        train_data, valid_data = data[train_index], data[valid_index]\n",
    "        \n",
    "        # scale data \n",
    "        scaled_train_data, scaled_valid_data, scaler = scale_train_valid_data(train_data, valid_data, scaling_method)\n",
    "        \n",
    "        # train model \n",
    "        model, history = train_model(scaled_train_data, \n",
    "                latent_dim, n_layer1_in_20s, n_layer2_in_25s, n_layer3_in_50s,\n",
    "                epochs = 500)\n",
    "\n",
    "        # evaluate on valid data\n",
    "        score = evaluate_model(model, scaled_valid_data)\n",
    "        \n",
    "        # Get the loss after the last training-epoch.        \n",
    "        losses.append(score['loss'])\n",
    "    \n",
    "    loss = np.mean(losses)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "\n",
    "    # Print the loss.\n",
    "    print(f\"trial vae loss: {loss}\")\n",
    "    print(f\"best vae loss: {best_loss}\")\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Trial run time: {np.round((end - start)/60.0, 2)} minutes\") \n",
    "    print('-------------------------------------------')   \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Time VAE HPO Loop, by dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-threaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cpus_to_use:  10\n",
      "iter: 0  mse:  507.9661458333333\n",
      "All done in 3.52 minutes!\n"
     ]
    }
   ],
   "source": [
    "num_cpus_to_use = max(multiprocessing.cpu_count() - 2, 1)\n",
    "print(\"num_cpus_to_use: \", num_cpus_to_use)\n",
    "\n",
    "# how many folds in cross validation\n",
    "num_folds = 5\n",
    "\n",
    "# num of trials for Bayesian search: initial and total (including initial)\n",
    "n_initial_points = max(5, num_cpus_to_use)\n",
    "n_calls = 60\n",
    "\n",
    "\n",
    "# our model name\n",
    "model = 'vae_conv_I'\n",
    "\n",
    "# dataset_names = ['sine', 'stocks', 'air', 'energy']\n",
    "percs = [2, 5, 10, 20, 100]\n",
    "\n",
    "\n",
    "# to custom run specific data\n",
    "dataset_names = ['sine']\n",
    "percs = [ 20 ]\n",
    "\n",
    "\n",
    "# set random gen seed for reproducibiity\n",
    "set_seeds(42)\n",
    "\n",
    "main_start_time = time.time()    \n",
    "\n",
    "for data_name in dataset_names:    \n",
    "    for p in percs:  \n",
    "        \n",
    "        # --------------------------------------------------------------------\n",
    "        ### file name to load\n",
    "        fname = f'{input_dir + data_name}_subsampled_train_perc_{p}.npz'\n",
    "        \n",
    "        ### read data        \n",
    "        loaded = np.load(fname)\n",
    "        data = loaded['data']  \n",
    "        # print(fname, data.shape) \n",
    "        \n",
    "        # --------------------------------------------------------------------        \n",
    "        # k-folds \n",
    "        kf = KFold(n_splits=num_folds)        \n",
    "        # --------------------------------------------------------------------\n",
    "        \n",
    "        optimizer = Optimizer(\n",
    "            dimensions = param_grid, # the hyperparameter space\n",
    "            base_estimator = \"GP\", # the surrogate\n",
    "            n_initial_points=n_initial_points, # the number of points to evaluate f(x) to start of\n",
    "            acq_func='EI', # the acquisition function\n",
    "            random_state=0, \n",
    "            n_jobs=num_cpus_to_use,\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        best_loss = 1e10\n",
    "        trial_num = 1\n",
    "        \n",
    "        num_loops = int(np.ceil(n_calls / num_cpus_to_use))\n",
    "        # print(f'num_loops: {num_loops}')\n",
    "\n",
    "        for i in range(num_loops):\n",
    "            x = optimizer.ask(n_points=num_cpus_to_use)  # x is a list of n_points points\n",
    "            y = Parallel(n_jobs=num_cpus_to_use)(delayed(objective)(v) for v in x)  # evaluate points in parallel\n",
    "            optimizer.tell(x, y)\n",
    "            print(\"iter:\", i, ' mse: ', optimizer.yi[-1])\n",
    "            break\n",
    "        \n",
    "        hpo_results = pd.concat([\n",
    "            pd.DataFrame(optimizer.Xi),\n",
    "            pd.Series(optimizer.yi),\n",
    "        ], axis=1)\n",
    "\n",
    "        hpo_results.columns = dim_names + ['loss']\n",
    "        \n",
    "        file_name = f'hpo_results_{model}_{data_name}_perc_{p}.csv'   # \n",
    "        hpo_results.to_csv(os.path.join( hpo_dir, file_name), index=False)\n",
    "                       \n",
    "        \n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = np.round((end - main_start_time)/60.0, 2)\n",
    "print(f\"All done in {elapsed_time} minutes!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect HPO Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'sine'\n",
    "test_perc = 2\n",
    "\n",
    "file_name = f'hpo_results_{model}_{test_data}_perc_{test_perc}.csv'   # \n",
    "hpt_results = pd.read_csv(os.path.join( hpo_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Score and Top Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function value at the minimum.\n",
    "# note that it is the negative of the accuracy\n",
    "\"Best score=%.4f\" % hpt_results['loss'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_results.sort_values(by='loss', inplace=True, ascending=False)\n",
    "hpt_results.reset_index(drop=True, inplace=True)\n",
    "hpt_results.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_results['loss'].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
