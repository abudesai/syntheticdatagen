{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "488e46a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "warnings.filterwarnings(action='ignore', message='Setting attributes')\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5b64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "config = tf.compat.v1.ConfigProto() # Another Version: config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1442b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot utils.\n",
    "def plot_scatter(*args, **kwargs):\n",
    "    plt.plot(*args, **kwargs)\n",
    "    plt.scatter(*args, **kwargs)\n",
    "\n",
    "\n",
    "# simple batcher.\n",
    "def data_generator(x, y, size):\n",
    "    assert len(x) == len(y)\n",
    "    batches = []\n",
    "    for ii in range(0, len(x), size):\n",
    "        batches.append((x[ii:ii + size], y[ii:ii + size]))\n",
    "    for batch in batches:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ecb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "  \"\"\"Min Max normalizer.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data\n",
    "  \n",
    "  Returns:\n",
    "    - norm_data: normalized data\n",
    "  \"\"\"\n",
    "  numerator = data - np.min(data, 0)\n",
    "  denominator = np.max(data, 0) - np.min(data, 0)\n",
    "  norm_data = numerator / (denominator + 1e-7)\n",
    "  return norm_data\n",
    "\n",
    "def sine_data_generation (no, seq_len, dim):\n",
    "  \"\"\"Sine data generation.\n",
    "  \n",
    "  Args:\n",
    "    - no: the number of samples\n",
    "    - seq_len: sequence length of the time-series\n",
    "    - dim: feature dimensions\n",
    "    \n",
    "  Returns:\n",
    "    - data: generated data\n",
    "  \"\"\"  \n",
    "  # Initialize the output\n",
    "  data = list()\n",
    "\n",
    "  # Generate sine data\n",
    "  for i in range(no):      \n",
    "    # Initialize each time-series\n",
    "    temp = list()\n",
    "    # For each feature\n",
    "    for k in range(dim):\n",
    "      # Randomly drawn frequency and phase\n",
    "      freq = np.random.uniform(0, 0.1)            \n",
    "      phase = np.random.uniform(0, 0.1)\n",
    "          \n",
    "      # Generate sine signal based on the drawn frequency and phase\n",
    "      temp_data = [np.sin(freq * j + phase) for j in range(seq_len)] \n",
    "      temp.append(temp_data)\n",
    "        \n",
    "    # Align row/column\n",
    "    temp = np.transpose(np.asarray(temp))        \n",
    "    # Normalize to [0,1]\n",
    "    temp = (temp + 1)*0.5\n",
    "    # Stack the generated data\n",
    "    data.append(temp)\n",
    "                \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fad2728",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real data shape: (10000, 24, 5)\n"
     ]
    }
   ],
   "source": [
    "num_samples, time_steps, input_dim, output_dim = 50_000, 15, 5, 5\n",
    "backend = NBeatsKeras(\n",
    "        input_dim=input_dim,\n",
    "        backcast_length=time_steps, forecast_length=5,\n",
    "        stack_types=(NBeatsKeras.GENERIC_BLOCK, NBeatsKeras.GENERIC_BLOCK),\n",
    "        nb_blocks_per_stack=2, thetas_dim=(4, 4), share_weights_in_stack=True,\n",
    "        hidden_layer_units=64\n",
    "    )\n",
    "\n",
    "# Definition of the objective function and the optimizer.\n",
    "backend.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "forecast_length = 5\n",
    "backcast_length = 3 * forecast_length\n",
    "\n",
    "'''\n",
    "milk = pd.read_csv('data/milk.csv', index_col=0, parse_dates=True)\n",
    "print(milk.head())\n",
    "milk = milk.values.flatten()  # just keep np array here for simplicity. milk.shape = 168\n",
    "'''\n",
    "\n",
    "seq_len = 24\n",
    "no, dim = 10000, 5\n",
    "ori_data = sine_data_generation(no, seq_len, dim)\n",
    "ori_data = np.asarray(ori_data)\n",
    "ori_data = np.squeeze(ori_data)\n",
    "print(\"real data shape:\", ori_data.shape) # (no, seq_len, dim)\n",
    "\n",
    "# x: data backcast/y: forecast generation.\n",
    "x, y = [], []\n",
    "for epoch in range(backcast_length, len(ori_data) - forecast_length): # range(15, len-5):\n",
    "    x.append(ori_data[epoch - backcast_length:epoch]) # x = backcast (0,15), (1,16)\n",
    "    y.append(ori_data[epoch:epoch + forecast_length]) # y = forecast (15,20), (16,21)\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "# split train/test.\n",
    "c = int(len(x) * 0.8)\n",
    "# print(\"the c size is: \", c) # c = 118\n",
    "x_train, y_train = x[:c], y[:c] # cut the first part to be training samples (80%)\n",
    "x_test, y_test = x[c:], y[c:] # use the rest part to be testing samples (20%)\n",
    "\n",
    "# normalization.\n",
    "norm_constant = np.max(x_train)\n",
    "x_train, y_train = x_train / norm_constant, y_train / norm_constant\n",
    "x_test, y_test = x_test / norm_constant, y_test / norm_constant\n",
    "test_size = len(x_test)\n",
    "# print(\"test_size is:\", test_size) # test_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b0f09f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7984, 15, 24, 5), (7984, 5, 24, 5), (1996, 15, 24, 5), (1996, 5, 24, 5))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the model data shape\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df1b22b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/1000\n",
      "1200/1200 [==============================] - 29s 10ms/step - loss: 0.3119 - val_loss: 0.3010\n",
      "Epoch 2/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3015 - val_loss: 0.3010\n",
      "Epoch 3/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3024\n",
      "Epoch 4/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3013 - val_loss: 0.3008\n",
      "Epoch 5/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3008\n",
      "Epoch 6/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3010\n",
      "Epoch 7/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3009\n",
      "Epoch 8/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3008\n",
      "Epoch 9/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3011\n",
      "Epoch 10/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3009\n",
      "Epoch 11/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3008\n",
      "Epoch 12/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3007\n",
      "Epoch 13/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 14/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 15/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 16/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 17/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 18/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3008\n",
      "Epoch 19/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3008\n",
      "Epoch 20/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 21/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3007\n",
      "Epoch 22/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3009\n",
      "Epoch 23/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 24/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 25/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3009\n",
      "Epoch 26/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 27/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3007\n",
      "Epoch 28/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3008\n",
      "Epoch 29/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 30/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 31/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 32/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3008\n",
      "Epoch 33/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 34/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 35/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 36/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 37/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 38/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 39/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 40/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 41/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 42/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 43/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 44/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3011\n",
      "Epoch 45/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 46/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 47/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 48/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 49/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 50/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 51/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 52/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 53/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 54/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3008\n",
      "Epoch 55/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 56/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 57/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 58/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 59/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 60/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 61/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 62/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 63/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 64/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 65/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 66/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 67/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 68/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 69/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 70/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 71/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 72/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 73/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 74/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 75/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 76/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 77/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 78/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 80/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 81/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 82/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3007\n",
      "Epoch 83/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 84/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 85/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 86/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 87/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 88/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 89/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 90/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 91/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3008\n",
      "Epoch 92/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 93/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 94/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 95/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 96/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3008\n",
      "Epoch 97/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3003 - val_loss: 0.3007\n",
      "Epoch 98/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 99/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 100/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 101/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3008\n",
      "Epoch 102/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3013 - val_loss: 0.3006\n",
      "Epoch 103/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 104/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 105/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 106/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3007\n",
      "Epoch 107/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 108/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 109/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 110/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 111/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 112/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 113/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 114/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 115/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 116/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 117/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 118/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 119/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 120/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 121/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 122/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 123/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 124/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3009\n",
      "Epoch 125/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 126/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 127/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 128/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 129/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 130/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 131/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 132/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 133/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 134/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3013 - val_loss: 0.3006\n",
      "Epoch 135/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 136/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 137/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 138/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 139/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 140/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 141/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 142/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 143/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 144/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3009\n",
      "Epoch 145/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 146/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 147/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 148/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 149/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3008\n",
      "Epoch 150/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 151/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 152/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 153/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 154/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 155/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 156/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 157/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 158/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 159/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 160/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 161/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 162/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 163/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 164/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 165/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 166/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 167/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 168/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 169/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 170/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 171/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 172/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 173/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 174/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 175/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 176/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 177/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 178/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 179/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 180/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.2999 - val_loss: 0.3006\n",
      "Epoch 181/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 182/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 183/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 184/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 185/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3001 - val_loss: 0.3007\n",
      "Epoch 186/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 187/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 188/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 189/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 190/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 191/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 192/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3007\n",
      "Epoch 193/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 194/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 195/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 196/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 197/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 198/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 199/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 200/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 201/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 202/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 203/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 204/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 205/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 206/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 207/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 208/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 209/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3008\n",
      "Epoch 210/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 211/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 212/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 213/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3007\n",
      "Epoch 214/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 215/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 216/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 217/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3001 - val_loss: 0.3006\n",
      "Epoch 218/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 219/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 220/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 221/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 222/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 223/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 224/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3008\n",
      "Epoch 225/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 226/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 227/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 228/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 229/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 230/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 231/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 232/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 234/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 235/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3007\n",
      "Epoch 236/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 237/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 238/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 239/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 240/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 241/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 242/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 243/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 244/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 245/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 246/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 247/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 248/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 249/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 250/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 251/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 252/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 253/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 254/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 255/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 256/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 257/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 258/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 259/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 260/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 261/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 262/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 263/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 264/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 265/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 266/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3005\n",
      "Epoch 267/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 268/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 269/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 270/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 271/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 272/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 273/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 274/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3008\n",
      "Epoch 275/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 276/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 277/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 278/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 279/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 280/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 281/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3008\n",
      "Epoch 282/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 283/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 284/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 285/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3005\n",
      "Epoch 286/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 287/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 288/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 289/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3007\n",
      "Epoch 290/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 291/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 292/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 293/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 294/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3005\n",
      "Epoch 295/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 296/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 297/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 298/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 299/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 300/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 301/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 302/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 303/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 304/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 305/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 306/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 307/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 308/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 309/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3007\n",
      "Epoch 310/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 311/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 312/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3001 - val_loss: 0.3006\n",
      "Epoch 313/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 314/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 315/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 316/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 317/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 318/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3005\n",
      "Epoch 319/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 320/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 321/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 322/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 323/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 324/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 325/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3005\n",
      "Epoch 326/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 327/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 328/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3009\n",
      "Epoch 329/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 330/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 331/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 332/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 333/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 334/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 335/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 336/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 337/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 338/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 339/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 340/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 341/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 342/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 343/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 344/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 345/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 346/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3007\n",
      "Epoch 347/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 348/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 349/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 350/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 351/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 352/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 353/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 354/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 355/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 356/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 357/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 358/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 359/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 360/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 361/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 362/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 363/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 364/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 365/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 366/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 367/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 368/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 369/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 370/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 371/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 372/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 373/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 374/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 375/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 376/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 377/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 378/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 379/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 380/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3001 - val_loss: 0.3005\n",
      "Epoch 381/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 382/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 383/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 384/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 385/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 386/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 387/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3008\n",
      "Epoch 388/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 389/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 390/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 391/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 392/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 393/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 394/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 395/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 396/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 397/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 398/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 399/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 400/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 401/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 402/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 403/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 404/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 405/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 406/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 407/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 408/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 409/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 410/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 411/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 412/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 413/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 414/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 415/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 416/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 417/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 418/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 419/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 420/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 421/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 422/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 423/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 424/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 425/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 426/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 427/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 428/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 429/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 430/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 431/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 432/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 433/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 434/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 435/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 436/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 437/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 438/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 439/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 440/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 441/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 442/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 443/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 444/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 445/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 446/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 447/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 448/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 449/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 450/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 451/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 452/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 453/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 454/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 455/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 456/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 457/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 458/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 459/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 460/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 461/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 462/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 463/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 464/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 465/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 466/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 467/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 468/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 469/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 470/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 471/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 472/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 473/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 474/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 475/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 476/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 477/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 478/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 479/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 480/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 481/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 482/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 483/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 484/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 485/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 486/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 487/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3007\n",
      "Epoch 488/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3008\n",
      "Epoch 489/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 490/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 491/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 492/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 493/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 494/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 495/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 496/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 497/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 498/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 499/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 500/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 501/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 502/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 503/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3007\n",
      "Epoch 504/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 505/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 506/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 507/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 508/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 509/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 510/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 511/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 512/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 513/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 514/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 515/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 516/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 517/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 518/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 519/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 520/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 521/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 522/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 523/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 524/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 525/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 526/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 527/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 528/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 529/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 530/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 531/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3005\n",
      "Epoch 532/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 533/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 534/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 535/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 536/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3000 - val_loss: 0.3005\n",
      "Epoch 537/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 538/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 539/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 540/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3008\n",
      "Epoch 541/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 542/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 543/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 544/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 545/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 546/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 547/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 548/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 549/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 550/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 551/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 552/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 553/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3007\n",
      "Epoch 554/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 555/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 556/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 557/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 558/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3005\n",
      "Epoch 559/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 560/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 561/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 562/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 563/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 564/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 565/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 566/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 567/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 568/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 569/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 570/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3009 - val_loss: 0.3008\n",
      "Epoch 571/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 572/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 573/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 574/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 575/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 576/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 577/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 578/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 579/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 580/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 581/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 582/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 583/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 584/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 585/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 586/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 587/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 588/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 589/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 590/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 591/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 592/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 593/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 594/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 595/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 596/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 597/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 598/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 599/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 600/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 601/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 602/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 603/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 604/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 605/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 606/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 607/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 608/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 609/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 610/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 611/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 612/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 613/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 614/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 615/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 616/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 617/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 618/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 619/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 620/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 621/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 622/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 623/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 624/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 625/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 626/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 627/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 628/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 629/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 630/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3005\n",
      "Epoch 631/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 632/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3008\n",
      "Epoch 633/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 634/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 635/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 636/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 637/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 638/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 639/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 640/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 641/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 642/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 643/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 644/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3005\n",
      "Epoch 645/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 646/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 647/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3007\n",
      "Epoch 648/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 649/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 650/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 651/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 652/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 653/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 654/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 655/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 656/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 657/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 658/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 659/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 660/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 661/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 662/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 663/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 664/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 665/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 666/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 667/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 668/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 669/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 670/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 671/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 672/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 673/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 674/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 675/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 676/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 677/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 678/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 679/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 680/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 681/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 682/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 683/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 684/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 685/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 686/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 687/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 688/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 689/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 690/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3008\n",
      "Epoch 691/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 692/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 693/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 694/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 695/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 696/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3008\n",
      "Epoch 697/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 698/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 699/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 700/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 701/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 702/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 703/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 704/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 705/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3005\n",
      "Epoch 706/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 707/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 708/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 709/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 710/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 711/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 712/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 713/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 714/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 715/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 716/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 717/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 718/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 719/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 720/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 721/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 722/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 723/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 724/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 725/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 726/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 727/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 728/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 729/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 730/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 731/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 732/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 733/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 734/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 735/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 736/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 737/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 738/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 739/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 740/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 741/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 742/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 743/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 744/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 745/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 746/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 747/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3005\n",
      "Epoch 748/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 749/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 750/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 751/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 752/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 753/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 754/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 755/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 756/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3008\n",
      "Epoch 757/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 758/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 759/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 760/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 761/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 762/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 763/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 764/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 765/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 766/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 767/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 768/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 769/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 770/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 771/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 772/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 773/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 774/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 775/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 776/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3007\n",
      "Epoch 777/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 778/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 779/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 780/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 781/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 782/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 783/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 784/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 785/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 786/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 787/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 788/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 789/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 790/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 791/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 792/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 793/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 794/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 795/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 796/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 797/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 798/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 799/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 800/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 801/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 802/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 803/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 804/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 805/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 806/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 807/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 808/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 809/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 810/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 811/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 812/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 813/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 814/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 815/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 816/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 817/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 818/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 819/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 820/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 821/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 822/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 823/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 824/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 825/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 826/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 827/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 828/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 829/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 830/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 831/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3007\n",
      "Epoch 832/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 833/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 834/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 835/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 836/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 837/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 838/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 839/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 840/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 841/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 842/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 843/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 844/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 845/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 846/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 847/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3009\n",
      "Epoch 848/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3007\n",
      "Epoch 849/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 850/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 851/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 852/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 853/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 854/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 855/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 856/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 857/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 858/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 859/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 860/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 861/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 862/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 863/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 864/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 865/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 866/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 867/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 868/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 869/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 870/1000\n",
      "1200/1200 [==============================] - 7s 6ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 871/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 872/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 873/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 874/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 875/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 876/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 877/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 878/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 879/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 880/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 881/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3005\n",
      "Epoch 882/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 883/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3009\n",
      "Epoch 884/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 885/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 886/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3004 - val_loss: 0.3007\n",
      "Epoch 887/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 888/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3005\n",
      "Epoch 889/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3007\n",
      "Epoch 890/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 891/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 892/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 893/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 894/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3006\n",
      "Epoch 895/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 896/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 897/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 898/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 899/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 900/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 901/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 902/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 903/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 904/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 905/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 906/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 907/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 908/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 909/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3007\n",
      "Epoch 910/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 911/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3005\n",
      "Epoch 912/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 913/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 914/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 915/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 916/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 917/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 918/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 919/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 920/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 921/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 922/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 923/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 924/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 925/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 926/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 927/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 928/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 929/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 930/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3007\n",
      "Epoch 931/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 932/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3005\n",
      "Epoch 933/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 934/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 935/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3006\n",
      "Epoch 936/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 937/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 938/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3003 - val_loss: 0.3006\n",
      "Epoch 939/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 940/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 941/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3007\n",
      "Epoch 942/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 943/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 944/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 945/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3007\n",
      "Epoch 946/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3001 - val_loss: 0.3007\n",
      "Epoch 947/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 948/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 949/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3002 - val_loss: 0.3006\n",
      "Epoch 950/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 951/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 952/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 953/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 954/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 955/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 956/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3007 - val_loss: 0.3008\n",
      "Epoch 957/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 958/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 959/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 960/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 961/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 962/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 963/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 964/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 965/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 966/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 967/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 968/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 969/1000\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3008 - val_loss: 0.3008\n",
      "Epoch 970/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 971/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 972/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 973/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3011 - val_loss: 0.3006\n",
      "Epoch 974/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 975/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 976/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3012 - val_loss: 0.3006\n",
      "Epoch 977/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 978/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 979/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 980/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3010 - val_loss: 0.3005\n",
      "Epoch 981/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3005\n",
      "Epoch 982/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 983/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3005\n",
      "Epoch 984/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 985/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n",
      "Epoch 986/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 987/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 988/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 989/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3006\n",
      "Epoch 990/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3008 - val_loss: 0.3007\n",
      "Epoch 991/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 992/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3009 - val_loss: 0.3006\n",
      "Epoch 993/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 994/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3005\n",
      "Epoch 995/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3006\n",
      "Epoch 996/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3004 - val_loss: 0.3006\n",
      "Epoch 997/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3000 - val_loss: 0.3006\n",
      "Epoch 998/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3005 - val_loss: 0.3006\n",
      "Epoch 999/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3006 - val_loss: 0.3005\n",
      "Epoch 1000/1000\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.3007 - val_loss: 0.3005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stufs1/zuwang/anaconda3/envs/nbeats/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction_forecast shape is: (47996, 5, 5)\n",
      "the prediction_backcast shape is: (47996, 15, 5)\n",
      "the reloaded prediction_shape is: (47996, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "print('Training...')\n",
    "backend.fit(x_train, y_train, validation_split=0.2, epochs=1000, batch_size=128)\n",
    "\n",
    "# Save the model for later.\n",
    "backend.save('n_beats_model.h5')\n",
    "\n",
    "# Predict on the testing set (forecast).\n",
    "predictions_forecast = backend.predict(x_test)\n",
    "print(\"the prediction_forecast shape is:\", predictions_forecast.shape) #shape: (30, 5, 1)\n",
    "#np.testing.assert_equal(predictions_forecast.shape, (test_size, backend.forecast_length, output_dim))\n",
    "\n",
    "# Predict on the testing set (backcast).\n",
    "predictions_backcast = backend.predict(x_test, return_backcast=True)\n",
    "print(\"the prediction_backcast shape is:\", predictions_backcast.shape) #shape: (30, 15, 1)\n",
    "#np.testing.assert_equal(predictions_backcast.shape, (test_size, backend.backcast_length, output_dim))\n",
    "\n",
    "# Load the model.\n",
    "model_2 = NBeatsKeras.load('n_beats_model.h5')\n",
    "predicts = model_2.predict(x_test)\n",
    "print(\"the reloaded prediction_shape is:\", predicts.shape) #shape: (30, 5, 1)\n",
    "#np.testing.assert_almost_equal(predictions_forecast, model_2.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af84fa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47996, 5, 5), (47996, 5, 5))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63789e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 5), (5, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[0].shape, y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "731f6da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38564227469937223"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(predicts[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb62e5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictive_score_mae: 0.3005697242592733\n",
      "predictive_score_mse: 0.13210614825862768\n"
     ]
    }
   ],
   "source": [
    "num_sample = len(x_test)\n",
    "\n",
    "MAE_temp = 0.0\n",
    "MSE_temp = 0.0\n",
    "\n",
    "for i in range(num_sample):\n",
    "    MAE_temp = MAE_temp + mean_absolute_error(y_test[i], predicts[i])\n",
    "    MSE_temp = MSE_temp + mean_squared_error(y_test[i], predicts[i])\n",
    "\n",
    "predictive_score_mae = MAE_temp/num_sample\n",
    "predictive_score_mse = MSE_temp/num_sample\n",
    "print(\"predictive_score_mae:\", predictive_score_mae)\n",
    "print(\"predictive_score_mse:\", predictive_score_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32fe2ec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2a73d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 5), (15, 5), (5, 5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff, xx, yy = predicts[0] * norm_constant, x_test[0] * norm_constant, y_test[0] * norm_constant\n",
    "ff.shape, xx.shape, yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08926534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5K0lEQVR4nO29fXhU1bX4/1kJr1FEKEo1kom3V2uxKBrUvtwqV63F9vG1L6KxBUWRqli17a0Yq6g3FRXfWm01IEUlFa23tXirtV5Kvj6/qi3QKijWl9oESLWIaDBECEnW7489J3MyOWdm8jZzZrI+z3OeOWfvfc5Zc7KzZp+1115LVBXDMAyjcCnKtQCGYRjGwGKK3jAMo8AxRW8YhlHgmKI3DMMocEzRG4ZhFDhDci1AMuPGjdPy8vLAuh07drDHHntkV6A0RE2mqMkD0ZNp7dq1W1V1n2zfN5/6dtTkgejJFDV5UvZrVY3UVlFRoWGsWrUqtC5XRE2mqMmjGj2ZgDVqfTslUZNHNXoyRU2eVP06I9ONiEwTkddE5E0RuSqg/g4ReTG+vS4iH/jq2n11K3rzS1VbC+XlsHat+6yt7c1VDMMw+oFUCsmrKyoKVlap6ntblwFpTTciUgzcA3wR2AysFpEVqrrBa6OqV/jazwWO8F3iI1Wd3COpfNTWwuzZ0NICu3YV0dDgjgEqK3t7VcMwjF7gV0hAF4UE4XWVlb0/N911MyATG/3RwJuq+haAiCwHTgM2hLQ/G7guo7tnQFVV4vvdfPMxgDuuqjJFbxhGlvEppNgzz7iylha45BJQTSgrj5YWmDMHVq2C5cuD688/3527e3dwnQjs2tW9rgdKMBNFXwps8h1vBo4JaigiMeBA4A++4hEisgZoAxao6uMB580GZgOMHz+eurq6zrq5c93nP/+5B8uXH8L27cOZOnUj06b9g7q63IdvaG5u7iJvromaPBBNmQyjV2zc2Ll74NNPJ8qbmsLPaW6G3/0OduwIrm9tDT83VZ1PlnT0t9fNdOAxVW33lcVUtVFE/g34g4isV9W/+09S1RqgBmDKlCk6derUzrqZM92bCsCPfvQsV199LHV1ZTz3XBkvvwwHHdTP36CH1NXV4Zc310RNHoimTIbRK8rKOhVS3S23MPW//suVx2Lu01NWfmIxqK93tvWw+lTnhtWVlWUsdiaTsY3ABN/xAfGyIKYDD/sLVLUx/vkWUEdX+31aqquhpMTtDxvWEf+EoUPhiCNg6VJYtqxP8xSGYRiZ4VdIRXH1WVLiyv11Hl5d8rnJ9b2ty5BMRvSrgYNE5ECcgp8OnJPcSEQOAcYAz/vKxgAtqrpLRMYBnwduyVg6Eiaoqir3GYu573fccXDuuXDeeVBcDO3xdwibrDUMY8AIU0h+ZVNV5cwqZWVd6/znBtX3pS4dYX6X/g34MvA68HegKl52A3Cqr818nA3ef97ngPXAS/HPWenu1RNf47Y21b33VnUzGV23WKw3nqg9J2q+tJGSZ9ky1VhMVy1c6P4gy5Z1q1OR/qvLpF7Njz4ToiaPavRkipo8qfp1RjZ6VX0SeDKp7Nqk4/kB5z0HTMrwN6fHFBeHz4H0YJ7CGAgGwpUsVV069zV7vTMGMZELgdBTfHMjXSgtzb4shg+fG9qk++93ZZ6rmUiwm5nnYtXTussug5074Qc/CK43X1xjkJP3Qc2C5inAuZ2uXp19eYw4vleqYdu3J8qbm+HDD4PPef99t/W0bts2uOACeO+9tLIYxmAk7xV9ZSXU1Lg5ERH3OX8+jBwJn/88zJjhyswjJ8v4XL/WXnFFojwWS7iMJXPAAW4Lq5swIbyuoQH23z+tLIYxGMl7RQ9O2dfXQ0eH+7zuOvjrX+HTn4YHH3QDOtWEydaUfRbojbvYggVuC6u76abwurIyuOWWPruheaSL7xRv8w0R2SAir4jIL3p8E8PIEnlvow9j7NjgN3kz2WaJvrih9bYuE/e1DMgkvpOIHATMAz6vqu+LyL49uolhZJGCVfQAmzYFl5vJNktUVrqtrs69agXVpTqvp3WZ1GdGJvGdLgTuUdX3AVR1S19vahgDRUEr+jCPnD33dCEkhg3LvkxGXpBJfKeDAUTkj0AxMF9Vf5d8oVRxnPxELR5Q1OSB6MkUNXlSUdCKvrq6q1s1wJAhzunj+ONh+nRYuLBPb/nG4GUIcBAwFRcW5FkRmaSqH/gbaYo4Tn6iFg8oavJA9GSKmjypKGhFH2ayHTLEeeP88Y+Jtra2xvCRSXynzcCfVHU38A8ReR2n+M2p14gcBeF1k4pkj5zKSjjrLDdZm4w3UWsMejrjO4nIMFx8p+TsaI/jRvPE4zgdDLyVRRkNI2MKXtGH8c47weU2UWuoahtwKfA08CrwqKq+IiI3iMip8WZPA++JyAZgFfB9VQ1ZsWUYuaWgTTepCJuoDVtzYwwu0sV3igeRujK+GUakGbQj+rDQCU1NsHJl9uUxDMMYKAatog8KnbBwoRvpn3SSq7fQCYZhFAKD1nQDwWtrZs+GE06AX/gWtJtHjmEY+cygHdGHMWoU/Otf3cvNI8cwjHzFFH0AFjrBMIxCwhR9AGFRbceMcVEwDcMw8glT9AEEeeQUFbn8FrNnu6QmhmEY+YIp+gCCPHKWLoWrr4bFi12cnHvucd44a9eaV45hGNFmUHvdpCIs2u3hh8O3vgXPP58w45hXjmEYUcZG9D3kG99wcXI8Jb927XjAvHIMwxg4atfXUn5nOUXXF1F+Zzm163tmQshI0adLqyYiM0XkXRF5Mb5d4KubISJvxLcZPZIuovjj5Dz88Kc6980rxzCM/qZ2fS2zn5hNQ1MDitLQ1MDsJ2b3SNmnVfS+tGonAxOBs0VkYkDTR1R1cnxbHD93LHAdLmnD0cB1IjImY+kiit8r53OfS0SvDctrbRiG0VuqVlbRsrulS1nL7haqVmZuQshkRN+ZVk1VWwEvrVomfAl4RlW3xVOuPQNMy1i6iOL3yjnzzDc6y3fvhr/9LUdCGYZRkGxsCjYVhJUHkclkbCZp1QC+KiLHAq8DV6jqppBzS5NPzLd0a6Wl8NBD0NgI++7bzI9/XEdT02huu+1Qpkwp4jvf2cDHP76tM11haWlw/PuBICrPyE8UZTKMfEBV2Wv4XjTtaupWVzY6ZMFPAP3ldfME8LCq7hKRi4AHgOMzPTlf062Bk+mss6YCzhvn2GPhRz86rEubkhLnrpkNj5yoPqOoyWQYUae1vZULn7iQpl1NFEsx7dreWVcytITqE6ozvlYmppu0adVU9T1V9ZYRLQYqMj23kCgrc5mskjGPHMMw0uH3rJlwxwQm3zuZB196kOunXs/S05cSGx1DEGKjY9ScUkPlpMxHjpmM6DvTquGU9HTgHH8DEdlPVd+OH56Ky8oDLgvPj3wTsCcB8zKWLg/ZvDm43DxyDMMIw/Os8SZdN293imTOlDlce5zLd3PuYef2+vppR/QZplW7TEReEZGXgMuAmfFztwE34n4sVgM3xMsKlrA4Ofvum105jL6RzqXY1+6rIqIiMiWb8hmFRZBnDcBTbzzVL9fPyI9eVZ9U1YNV9ROqWh0vu1ZVV8T356nqoap6uKr+p6r+zXfuElX99/j2836ROsIExckRga1b4d57cyOT0TMydSkWkVHAd4A/ZVdCo9DoD8+aVNjK2H4mKE7OvffCl74E3/62S2pimasiT6YuxTcCNwM7symcUVh4njVB9MSzJhUW62YACIqTM2sWnHEGPPFEosxi5ESWtC7FInIkMEFVfysi3w+7UL65DntETR6Inkz9IU9rRyu3vnYrTbuamLLXFL6279cYUuTUcpEUERsd65fvbIo+SxQXw7p13cs9jxxT9PmDiBQBtxOfi0pFvroOR00eiJ5MfZXn3R3vcvojp/PclueoPr6astFlXPOHa9jYtJGy0WVUn1DNmZPO7BdZTdFnkTDPG/PIiRzp3IJHAZ8G6kQE4OPAChE5VVXXZE1KI++oXV9L1coqGpoaGCJDEBEe/dqjfP3QrwN986xJhdnos0iYR86ee7rwCUZk6HQpFpFhOJfiFV6lqjap6jhVLVfVcuAFwJS8kRJ/cDKANm2jSIpo7Wgd8Hubos8iQR45Q4bAhx/CiSfCli25kcvoSoYuxYbRI67+v6u7uVDuat/Vo+BkvcUUfRYJy1y1bBn8+c9QUQE33ui8ccwrJ7ekcylOajvVRvNGKlrbW9m4fWBdKFNhNvosE5a5auJEOOkkuPbaRJl55RhG/vPujnf56qNfDa3vLxfKVNiIPiIccQSMGNG93OLkGEb+8tI7L3HUoqNY/c/VXHzUxZQM7Wq77Wlwst5iI/oI0RgS7s28cgwjP/C8ajY2bWRcyTi279rOx0o+xrMzn+Wo0qP43ITPddZ7LpQ9CU7WW0zRR4iyMmeuSWaffbIvi2EYPSM5MNm7Le9SJEXM+495HFV6FACVkyqzotiTMdNNhAiLk/Puu3DLLW7S1iZqDSOaBAUm69AOFj63MEcSJbARfYTwJlyrqpy5pqwMrrkGfv97+MEP3Ora9njuAZuoNYxo4fnHJ5MNr5p02Ig+YlRWQn29S2BSXw8XXACPPAJjxiSUvIdN1BpGNFjx2goECazLhldNOkzR5wEi8MEHwXU2UWsY2cXLBLX27bXE7ohxxiNncNry0yjfu5wRQ7q6zmXLqyYdpujzhLDwCQcckF05DGMw4w9j8FH7R2zcvpHH//Y4Xyj7Aq9c/AqLT13cp5R/A4XZ6POE6mpnk29JSkKjCq++Cp/6VG7kMozBhH/C9c6Nd3aWN3zQwMihI3PmVZMOG9HnCUHhE37wA9i1C6ZMcUlNysth7VrzyDGMgcI/seoPRrZp+6ag5pHBFH0ekTxRu2ABvPiiM+vce6/zxNm9u6jTI8eUvWH0L/6J1StiVwSWRxFT9HnO/vt3NefcddeRgHnkGMZAUH1CdWcYg72GuPR/UZlwTYUp+gJgk++tsbl5WOd+0CpbwzDSU1sbvDixclIlNafUEBsdA4jUhGsqMlL0IjJNRF4TkTdF5KqA+itFZIOIrBORlSIS89W1i8iL8a1biFej7/g9cr773dWd+yNHwtatORDIMPKY2lpn+mxocM4OyabQykmV1F9eT8V+FdRfXh95JQ8ZKHoRKQbuAU4GJgJni8jEpGZ/Baao6mHAY8AtvrqPVHVyfLOkDQOAP3TCqFEuVdXQoS5r1WGHwbx5FjrBMDKlqqq7d1u+m0IzGdEfDbypqm+paiuwHDjN30BVV6mq92hewOXYNLKE3yMH3OfPfw5r1jgPnQULwkcnhmEk6OgIN3nm8+LETPzoSwG/79Bm4JgU7WcBT/mOR4jIGqANWKCqjyefICKzgdkA48ePp66uLvDCzc3NoXW5IioylZa6bFXNzc0sXVoHwPvvw5VXFvE///MJnn++lNLSDznnnFcZP76FbdsgW2JH5Rn1BBGZBtwFFAOLVXVBUv2VwAW4fv0ucL6q2qxIHvP22zBzZnh92KLFfKBfF0yJyLnAFOA4X3FMVRtF5N+AP4jIelX9u/88Va0BagCmTJmiU6dODbx+XV0dYXW5ImoyJctz/PFuJA/Q2DiKW289urPOK8+2TFHHZ678Im5gs1pEVqjqBl8zz1zZIiLfxpkrz8q+tEZvqa1NBBAcNw4++sjFkzrvPFi+3B17lJQ4E2m+konpphGY4Ds+IF7WBRE5EagCTlXVXV65qjbGP98C6oAj+iCv0UPCRiHDh8Obb2ZXljzCzJUFTvKE67vvwo4dcP31sGQJLFrUdXFiTU1+R4nNZES/GjhIRA7EKfjpwDn+BiJyBHAfME1Vt/jKxwAtqrpLRMYBn6frRK0xwASFThg2zIU8PvxwuPVWGDUKfvjDRGjk6ur87tT9QF/NlZ3kq1kyavJA/8q0bRucf/5ePPLIIWzdOpLjjtvEtGn/YMQIpa4uYQr1k3zrKD6jUFQ17QZ8GXgd+DtQFS+7ATd6B/g/4F/Ai/FtRbz8c8B64KX456x096qoqNAwVq1aFVqXK6ImU5A8y5apxmKqIu5z2TLVTZtUv/QlVVAtKnKf3lZS4toMpEy5BFijqfv713B2ee/4m8DdIW3PxY3oh6e6puZZ346aPKr9J1Nzc9f+7t9Esi9Pf5GqX2dko1fVJ4Enk8qu9e2fGHLec8CkTO5hDByVlcEj9KeecrbJbdu6lnuuZIN4VN9Tc+Vx6jNXGtFl1SqX4yGMfJ5wTYWtjB3EiDjPnCDy2ZWsH+g0V4rIMJy5sstiP5+58lT1mSuN6OBf3VpWBiee6JwTiopc5rbktJ35PuGaCgtTPMgJS0i+116wcyeMGNG9rtBR1TYRuRR4GudeuURVXxGRG3CvxyuAW4E9gV+KCMBGtQWBkcGbbPXmpjZtctvJJ8NjjzmlfsghXdN2FvLclCn6QU7QZG1xMTQ1ucnar33N/dMMhn8GP701VxrRIGh1K8CGDYmRfJhJsxAx080gJyjO/QMPuITkH3wAP/qRrao18ovduwtzdWtfMEVvdItzX1kJX/yi87VPJt9jfhiFRXKUyauvdm+iYRTqZGs6TNEboWzeHFxu4Y+NKBAUZfKmm1zE1iuvHFyTrekwRW+Ekmr0c8YZcPvtFhXTyB1hdvgRI+C227qbJPN9dWtfsMlYI5SgidqRI+GUU+A3v4HHH0+Ue/Z7GLz/TEb2SGWH995EB9NkazpsRG+EEjRRu2gRPPKIW2iVjNnvjYHAs8OvXev64KWXwqc+Fd5+sNrhU2GK3khJ0EQtwD//Gdx+sHo1GAOD3w7/2mtj2LgR7rkHWlvhu991b5h+BrMdPhWm6I1eETZq8pfXrq+l/M5y1r69lvI7y6ldb0Z8o2dcfXXCdLhoUcKdRgQWLiy8KJMDhSl6o1f40xd6+EdTtetrmf3EbBqaGujQDhqaGpj9xGxT9kYgyW6SDz4IDz/c9Q3xtNPe6NzfFI8tGvbGaXTFFL3RK4Ls9/7RVNXKKlp2u6HYwoaFALTsbqFqpRnxja4EuUnOnAnnnANDfO4iX/hCIq6c2eF7hil6o9ekGk1tbEoMxYZI4r+1oamBbR9t6zTrFF1fZGadQU6Qm6Qq7LOPSwJi/vB9xxS9MSCUjU4Mua4ou6JL3f637c/Mx2fS0NSAombWGQQkm2Zqa6GtDf73f8PdJLduhW9+s3vie7PD9xxT9MaAUH1CNSVD3VAsHt2RkqEl3HTCTQwpGkJbR1uX9oPVrON3HSzURWdhppl993VrMopCtJBnnvHeHCsqzA7fW0zRGwNC5aRKak6pITbaDcVio2PUnFLDVf9xVaftPpmGpgZef+/1QWPW8StAyP+gcUGjdgg2zbS1uTDYv/61mWeygSl6Y8ConFRJ/eX1VOxXQf3l9VROckMxv1knmU/e/Um+9etvDQqzjl8B/vOfewBdF52FKc4oEjRqv+ACOPfccNPMzp1w+ukwY4aFKxhoTNEbWcdv1vEoGVrCT07+CXuP2JsO7ehS17K7haueuQqgoEb7ftfB228/qnO/oQHuvru74ozCaL8no/adO1390KHB1/J7zpib5MBiit7IOn6zjiCdZp1Lj76Upp1Ngeds/nAzh/70UM57/LyCGe37Fd1ZZ/2tS93cud0VZ3KIiYEY8aeaMwgatc+a5UblqSKa/vznZprJNabojZzgmXU6ruvIyKyz1/C9eOO9N9jdsbtLuX+0D/034heRaSLymoi8KSJXBdQPF5FH4vV/EpHynt7Dv+jsqKPeAdyS/lQKsKEBvv99uOQSZxoJG/Gn+hEIqwubM3jgAXjjDbjiiu4/Prt2uQB3w4YFyxuLpV9zYQw8GUWvFJFpwF24/JmLVXVBUv1w4EGgAngPOEtV6+N184BZQDtwmao+3W/SGwVH9QnVzH5idpcJ25KhJfz0Kz/lm7/6ZuA5mz/czMR7JvLxPT/OHzf9kdb2VoDOET/Q+UOSCSJSDNwDfBHYDKwWkRWqusHXbBbwvqr+u4hMB24GzurBV00sLouP0mOxRKrGmprgUfKwYfDjH7tYL8m0tMDFF8NTT8Evf5lo49nLt251aSL/67/go48SdbNmwfr1cP/9CUW+YsUnOq85c2b677JkSfdIp/5Ru0WSzC1pR/S+Tn8yMBE4W0QmJjXr7PTAHbhOT7zddOBQYBrw0/j1DCOQMLNO5aTK0NH+3sP35sAxB1JXX9ep5D166bZ5NPCmqr6lqq3AcuC0pDanAQ/E9x8DThDPj7QHhLkOhoWYWLIEmpvDr7d9uxuZJ/8Q7NwJl1/uTEKekvfYtQtuvtn9EHg899z+XdosXQof/3jwPW3UHn0yGdF3dnoAEfE6vX90cxowP77/GHB3vNOfBixX1V3AP0Tkzfj1nu8f8Y1CpHJSZeAIPGy0f/dX7qZyUiVyfbCe9a/SzZBSYJPveDNwTFgbVW0TkSbgY8BWfyMRmQ3MBhg/fjx1dXWBN2xubu5SV1oKDz0EjY1OaQ8b5srGjoU//jF8VD90KDQ1DaGpaTjbtw9j165i2tuL6OgQ2tu9zR2LKCUlbYwc6ba99trNkCFu/8ADt9PYuCfg7h2LuaiRDQ1uwtSjqMjV1dU5+ZYu7SpPyNftFcnPKNdETZ5UZKLo+9LpS4EXks4t7bW0xqDGU/5VK6vY2LSRstFlVJ9Q3VkeGx2joam7vSOVO+dAo6o1QA3AlClTdOrUqYHt6urqCKsLorEx2FRSU+MyfwWZfbzVpWF1/kQzCxfW8b3vTe28pidaba0zNW3c6CaTq6vhzDMzFrtP9PQZDTRRkycVkcgw1dtRTxSImkxRkwf6V6ZSSlk6eWmi4D06r337J2/vjJbpUSRFxEbHenr/RmCC7/iAeFlQm80iMgQY7aTJDn77vl/peuWp7OVhdanmDPz3NXNMHqKqKTfgs8DTvuN5wLykNk8Dn43vD8G9vkpyW3+7sK2iokLDWLVqVWhdroiaTFGTRzW7Mi1bt0xjd8RU5ovG7ojpsnXLurUB1mjqPj8EeAs4EBgGvAQcmtTmEuDe+P504NFU19Qs9+1ly1RjMVUR97lsWWZ1AyVPfxA1maImT6p+La4+nPho5XXgBNwoZjVwjqq+4mtzCTBJVefEPRDOVNVviMihwC9wdvn9gZXAQaranuJ+7wJhXrnjSLKBRoCoyRQ1eSB6MsVUdZ9UDUTky8CdOE+zJapaLSI34P6ZVojICOAh4AhgGzBd4/NYKa6ZT307avJA9GSKmjyh/Tqtooe+dXoRqQLOB9qAy1X1qd5+CxFZo6pTenv+QBA1maImD0RTpqgRtWcUNXkgejJFTZ5UZGSjV9UngSeTyq717e8Evh5ybjVga+AMwzByhK2MNQzDKHDyTdHX5FqAAKImU9TkgWjKFDWi9oyiJg9ET6aoyRNKRjZ6wzAMI3/JtxG9YRiG0UNM0RuGYRQ4eaPo04WNzYE89SKyXkReFJE1OZJhiYhsEZGXfWVjReQZEXkj/jkmx/LMF5HG+HN6Me6qa8SJWr8G69s9kCdv+nZeKPoMI2jmgv9U1ck59KVdiosK6ucqYKWqHoRboJZN5REkD8Ad8ec0Oe6qaxDpfg3WtzORB/Kkb+eFoiezsLGDDlV9FrdAzY8/fO4DwOk5lscIx/p1CNa3+5d8UfRBETRzHQVTgd+LyNp4ULaoMF5V347vvwOMz6UwcS4VkXXx19+svW7nAVHs12B9uyfkRd/OF0UfRf5DVY/EvXZfIiLH5lqgZOKBjnLtP/sz4BPAZOBt4LacSmNkgvXtzMibvp0vij6TsLFZRVUb459bgF/jXsOjwL9EZD+A+OeWXAqjqv9S1XZV7QAWEZ3nFAUi16/B+nam5FPfzhdFvxo4SEQOFJFhuLCwK3IljIjsISKjvH3gJODl1GdljRXAjPj+DOA3OZTF+4f0OIPoPKcoEKl+Dda3e0I+9e1IJB5Jh7qsVZfi4tl7ETRfSXPaQDIe+HU8RegQ4Beq+rtsCyEiDwNTgXEishm4DlgAPCois3Ahcb+RY3mmishk3Gt2PXBRtuSJOhHs12B9uyfy5E3fthAIhmEYBU6+mG4MwzCMXmKK3jAMo8BJq+iDlv4m1YuI/Di+hHudiBzpq5sRX678hojMCDrfMAzDGFgyyRl7LNAMPKiqnw6o/zIwF/gycAxwl6oeIyJjgTXAFNxkxVqgQlXfT3W/cePGaXl5eWDdjh072GOPPdJ9p6wSNZmiJg9ET6a1a9duTZczdiDIp74dNXkgejJFTZ6U/Tosa7h2zXhfDrwcUncfcLbv+DVgP+Bs4L6wdmFbRUWFJuNlrV+4cFVo1vpcEbVM8FGTRzV6MuFyHWfU9/tzC+rbHlF7RlGTRzVCMsUV0qqFCzVKCilVv+4P98qwZdwZL++OL7OeDTB+/Hjq6uo667Ztgy1bYO5cOOCAZubOrWPLFvjVr2Ds2H6Qvo80Nzd3kTfXRE0eiKZMhtEramth9mxoaXHHDQ3uGKCy0tVXVcHGjVBWBtXVrtx/fqr6gSLsF8C/kXpE/7+4JdPe8UqcueZ7wDW+8h8C30t3r+RRTyymCm6bNeulzv1YLNHGG/GLZP8HNjKjjDhRk0c1ejJhI/q0RE0e1YjI5FNIL154YUI5HXCA6tKlqiUliTJwx55CWrYsfX0fFFmqft0fI/qwZdyNuAUG/vK6nl5848bE/v33H9a539AAf/oTvP46zJkT/gNrGIbRb/gU0uGLFiXKN2+GmTO7t29pcQrqlVfg3nsTispfX1Xl9lO9KfSR/nCvXAF8K+598xmgSV2EuaeBk0RkTDyq20nxsh5RVpbYnz37pS51n/mMe7apnp1hGEa/4VNIf7n00kT5mBSBK5ub4ZZb4P0QP5SGBrjootSKrLYWysuhqMh91tb2SOxM3CsfBp4HPikim0VklojMEZE58SZPAm8Bb+IC+1wMoKrbgBtx8TxWAzfEy3pEdTWUlLj9gw92D6qkBH76U1i+HDo6gs9raHCffXw+hmEYCXwKabvnQVVSAj/5CcRiwefEYrB7N0yYEFw/fDjs2BFc19AA558Ps2a5fdXEaL8Hyiyt6UZVz05Tr8AlIXVLgCUZSxOA99bi/bDFYl3nL37wg4RST2bSJGfaaW11x2bWMQyjT6RTSH7zC7gfgepqEIGbbgqur6lx1wtSZEVF8POfdy/3RvsZKrK8WBlbWQn19VBR4T79380/4vcYMQJOOw3+9reEkvcws45hGH0iTCFVVjqlHYs5xR6LueNM6oMUWUkJPPAAofgnMNOQF4o+FUHPbvFiePxxaGsLPqehAbZuNbOOYRj9jPcj0NHRfVSaqj7sR+Dcc8NNQv4JzDTkRZjidFRWBr/BxGLhZp3993fPur3dHZtZxzCMnBKmyKqrw01CGZL3I/pUhL0N3XSTM+94St7DzDqGYUSOdCahDChoRR/2fK66ynk8BdHQALt2mVnHMIwIkc4klIaCMN2kIuxtqKws3Kyz337uh2D3bndsZh3DMPKZgh7RpyLIrDNypHPXbGlJKHkPM+sMLkRkmoi8Fg+/fVVIm2+IyAYReUVEfpFtGQ0jUwatog8y6yxaBAsWdHfJ9Ah7AzAKCxEpBu4BTgYmAmeLyMSkNgcB84DPq+qhwOXZltMwMmXQKnoIN3uFeS0VF8ODD7pJXM+Gv3at2fALkKOBN1X1LVVtBZYDpyW1uRC4R+P5FVR1S5ZlNIyMGdSKPowgs87w4e4HYMYMp9i9FcnQqxXJRrTJJMT2wcDBIvJHEXlBRKZlTTrD6CEFPxnbG/yrnP1ho88+Gx57zNV7i7E2bhwF9HhFspH/DAEOwkVoPQB4VkQmqeoH/kapci34iVrM/qjJA9GTKWrypMIUfQhh3jrf+AacdVbi+Mc/rujc78GKZCPahIXe9rMZ+JOq7gb+ISKv4xT/an8jVa0BagCmTJmiU6dODbxhXV0dYXW5IGryQPRkipo8qTDTTS/wr0g+7riEdt977/CwC0ZesRo4SEQOFJFhwHRcOG4/jxPPtyAi43CmnLeyKKNhZIwp+l7gt+Gfcor73y4qcuGmjzwSrrnGFlvlM6raBlyKy5/wKvCoqr4iIjeIyKnxZk8D74nIBmAV8H1VfS83EhtGasx00wuCIpX+93/DHnu4SVl/CApbbJWfqOqTuFwL/rJrffsKXBnfDCPS2Ii+lyRHKj33XDjjDLfoKhlbbGUYRi4xRd/PbN4cXG6LrQzDyBWm6PuZVCGib7jB5REw+71hGNnEbPT9TFDo6JEjYfJkuO46F25B1ZWb/d4wjGxgI/p+JiyGznPPwT77JJS8h9nvDcMYaGxEPwCELbbaujW4vS20MgxjIMloRJ8uZKuI3CEiL8a310XkA19du68uedHJoCLMfj9qVHjETMMwjL6SVtFnErJVVa9Q1cmqOhn4CfArX/VHXp2qnsogJihYWnExbN8ORx8N69ZZZivDMPqfTEw3nSFbAUTEC9m6IaT92cB1/SNeYREWLG2vveDCC92q2qIiy2xlGEb/komiDwrZekxQQxGJAQcCf/AVjxCRNUAbsEBVHw84Ly8j/EHPZSothaVLu5ffd99QrrvuYF56aR/Ky5uYPv1Vxo3bCcC2bZDpLQrhGRmG0b/092TsdOAxVW33lcVUtVFE/g34g4isV9W/+0/K1wh/0L8ynX66+6yvH82CBZ/pLBdxyVGyLU9/EUWZDGMwkclkbCYhWz2mAw/7C1S1Mf75FlAHHNFjKQcJ/qiYfg44ILtyGIZRWGSi6DMJ2YqIHAKMAZ73lY0RkeHx/XHA5wm37Q96giZrAXbtgj//2SZqDcPoHWlNN6raJiJeyNZiYIkXshVYo6qe0p8OLI9H9fP4FHCfiHTgflQWqKop+hCCJmu/9S0XNuFzn7OJWsMwekdGNvp0IVvjx/MDznsOmNQH+QYdQYutrrjCmW/8YRXA0hcahpEZFgIhDxgzpruS97BVtYZhpMMUfZ4QNlE7YUJwuWEYhocp+jwhbKJ2jz3g7bezL0+hky7sh6/dV0VERWRKNuUzjJ5gij5PCIqKOXu2y251xBEwb57zxFm71jxy+komYT/i7UYB3wH+lF0JDaNnmKLPI7z0hR0d7vO++2D1ahcvZ8EC54nT0ZHwyDFl32s6w36oaivghf1I5kbgZmBnNoUzjJ5iYYrznEMPdYreY9GiwwHzyOkjacN+iMiRwARV/a2IfD/sQvka3iNq8kD0ZIqaPKkwRV8A+PPU/uMfe3Xum0fOwCAiRcDtwMx0bfM1vEfU5IHoyRQ1eVJhppsCwB/nfu7cv3bu771394xWRkakC/sxCvg0UCci9cBngBU2IWtEFVP0BYDfI6e0tBlwq2jffx/OPRd27MihcPlJyrAfqtqkquNUtVxVy4EXgFNVdU1uxDWM1JiiLwD8HjngPpcuhf/+b3j4YTjkEBce2WLkZIaqtgFe2I9XgUe9sB8iMqiT5xj5idnoCwQvdEJdnfPI8fjwQ7j55sSxxcjJjEzCfvjKp2ZDJsPoLTaiL3CWL+9e5nnkGIYxODBFX+CEed40NGRXDsMwcocp+gLH75Hjp7gY/vrX4DrDMAoLU/QFTlCMnOHDYfRoF+P+oYcsoYlhFDo2GVvgBCUzqa6GL34RzjrLJTYZMgTa2lw7m6w1jMLDRvSDgOQYOZWVsO++8MwzMGpUQsl72GStYRQWpugHMUOGQHNzcJ2FTzCMwsEU/SAnbLI2rNwwjPzDFP0gJyyhyb//O7S2Zl8ewzD6n4wUfbpsOyIyU0TeFZEX49sFvroZIvJGfJvRn8IbfSc5oUlZGXzlK7ByJRx/PNxzj3nkGEa+k9brxpdt54u4uNyrRWSFqm5IavqIql6adO5Y4DpgCqDA2vi57/eL9Ea/4IVP8PPIIzBjBjz3XCICpnnkGEZ+ksmIPtNsO0F8CXhGVbfFlfszwLTeiWpkk7POgrFju4c5No8cw8g/MlH0Qdl2SgPafVVE1onIYyLixfLO9FwjgrzzTnC5eeQYRn7RXwumngAeVtVdInIR8ABwfKYn52u6NYieTP0pz113uQnZ9nbhqacOpK6ujPLyJi644BXq6jKfqY3aMzKMwUYmij5dth1U9T3f4WLgFt+5U5POrUu+Qb6mW4PoydSf8jQ2Opt8S0uirL5+NLfd9jlOPBGOOSb83IGSyTAGI7Xra6laWcXGpo2UjS6j+oRqKidlPlGWiekmZbYdABHZz3d4Ki5ZA7jEDSeJyBgRGQOcFC8z8oBkj5xYzLlj7r03HHssXHiheeQYxkBTu76W2U/MpqGpAUVpaGpg9hOzqV2f+T9c2hG9qraJiJdtpxhY4mXbAdao6grgsnjmnTZgG/Gkyaq6TURuxP1YANygqtt68iWN3BLkkXPRRTB1KixenCgzjxzDGBiqVlbRsrulS1nL7haqVlZlPKrPyEafLtuOqs4D5oWcuwRYkpE0Rl7wsY/B9u3dyz2PnEJQ9CIyDbgLN7hZrKoLkuqvBC7ADW7eBc5XVYvyb/Q7DU3B3WpjU+ZeEbYy1ugVmzYFlxeCR45v7cjJwETgbBGZmNTsr8AUVT0MeIzEvJRh9BubmjYxYsiIwLqy0ZnHKTFFb/SKAo+Rk3btiKquUlXvffoFnKNBj/FyAaxda/Mcg53a9bWU31lO0fVFxO6I8e3ffptJP5uEqjK0aGiXtiVDS6g+oTrja1s8eqNXVFd398gpKXHlBUDQ+o9UPkazgKeCKlK5Dm/bBlu2wCWXwAEHNDN3bh1btsCvfuUWq+WSKLrERk2m/pRn20fb2NK0hbnj59L8sWb+Z8v/cO+aezlkz0O45vBrGMlIGj9spLW9lWHFwygdVcrY98ZmfH9T9EavCEto4rfPey5hc8fPZeadM3vsEpYPiMi5uBAfxwXVp3IdLi9P5O797Gcbef55t5YwFnN5A3JJFF1ioyZTf8pTfmd5oC2+paiFypP7/j9jit7oNUEeOR6eS1jL7hYYT6dLGJAPyj7t2hEAETkRqAKOU9VdPb2Jfz7jhRf2Dyw3BgdhE66btodMhvUQs9EbA4LfJezZ958FEi5heUAma0eOAO4DTlXVLb25iX8+47LL/tK5P2KEW6xmFD4d2sFPV/8UQQLrezLhmgpT9MaA4Hf9WvHuisDyqKKqbYC3duRV4FFv7Uh8vQjArcCewC/joblXhFwuFH8ugAkTPgRg6FCX2vFTn4KZM50ZxxakFQ7+Cdf9b9ufQ+4+hEuevIRD9z20m3dNTydcU2GK3hgQ/CORL4/7cuf+fnvuF9Q8cqjqk6p6sKp+QlWr42XXxhcIoqonqup4VZ0c305NfcXu+Fceg/v8+c/h1Vfd/gMPODOOamJBmin7/CV5hevbzW/zxrY3mDNlDuvmrGPxqYuJjY4hCLHRMWpOqek3M6cpemNAqD6hmpKhbrh6/NhEfLttO7fx2IbHuoxsyu8s79Fy7kLCS9xeUZFI3P6JT6RekGbkJ1f/39XdVrgCPPXGU4gIlZMqqb+8no7rOqi/vL5f57JsMtYYELxO6tnkY6NjXPnZK3n45Yf5+i+/zpCiIbR1tAF5N1GbFcIWpDU0QHs7FBdnVx6jb7y85WU2bg82W2bDnGkjemPA8EYoFftVUH95PZcdcxn/b+b/Y9SwUZ1K3iOPJmqzQqqFZ0ceCatWJRZbmQ0/OiS/qd675l4u/u3FHH7v4RRJsLrtrwnXVNiI3sgqw4qH0dzaHFiXDxO12SJoQdrIkS5i6G9+4/L5Fhe70T1YULko0MWlGPem+u3ffpsiKeKSoy5h4j4T+e7vv9vFfNOfE66psBG9kXXCRjAT9poQWD4YCQoRvWiRSwbz6qswenRCyXuYDT+3BEWZBBi/x3h+fPKPmTNlDjWn1AzYhGsqbERvZJ3qE6q7jHw89h65N1tbtjKuZFyOJIsWYQvSRo4MnqyFxEpbI7uoauiip3eaEzk5KydV5mQeykb0RtapnFTZZWRTtlcZ500+j9e2vsbkeydzXd115pGThjAbvghcc417GzD7/cDg2eHXvr2W2B0xvvv771JRUxHaPhs2+HSYojcGjhShGSvXQf2d0HE9NNwlLGk+gednPc/ujt3cWHcD5zzRwO75St38Bv7vxvMSyj7dDOQgmaH0L7byGDECjj7a1V10kRvdmw9+/+L3hX91x6ts3L6R25+/ncYPG5kzZQ4jh4zs0j5bNvh0mKI3BobaWqddPFuCX9v463ya6Ig/bOD0dbs59W9w07FwxtkwqhXufnw3f7r5O6HndWqw2lo3WzkINFyQDX/xYnjhBdgvYE2a2e/7B78d/v7G+zvLRxSP4Gdf+RmLTl2UExt8WlQ1UltFRYWGsWrVqtC6XBE1mSIjTyym6tStfrjffp37OmSIanFx4ti/FRfrbkE7QD8cijbuib6zB7qzGG0TVIuKgs8TUR0+PLgOnCw+cCkwC7Zvi4Q/imefVX3oIfdIRNznsmUDK09/kU2Zli0LfkYyX5T5KPPRr/30a537Ml+yJlsYqfq1jeiNgcEXgnGnP7h6W1t3dxGP9naKFQTYczcMa4e6GDxwOPxlP+jo6Ag+TxUuvzwjWQYDYfb7oiKX1H3GjEHx0tNrUr04+u3tn9n7M537UbDDp8IUvTEw+LTNy+edlyiPxRLBXZKJxdix38c6D8d9BF95E/50ABw9G77w7eHU7x18HgsWhF+3QNJeZUqQ/b6kxLlnjh0Lyb+XfrOOZbyCefO6rl+AxDPyh/bwiIodPhWm6I2BIUzbVFenrNvz1rtoGzGss3jPVrjv98P4xfiLeWm8MnkO/NKXvXXHUPj/5nw5/T0HEUH2+5oaOP98eP/94HMaGtx53hSHV1bIo33/vH0sBt/7Hpx1Vup8yH6PMSBadvhUhNl0/BswDXgNeBO4KqD+SmADsA5YCcR8de3Ai/FtRbp7mY2+b0RKnrihc9XChd2NwWFG0BR1+9+2v37s+84mOusU9OVx6NlnorE7YpldNw4FbqNPhW/qpMs2bFjX43POeSVsiiNn9OczWrZMdeTI7s9h1Ci3ZTDVE63/Ne2jjV5EioF7gJOBicDZIjIxqdlfgSmqehjwGHCLr+4j7UMoVyOPCQrNmFzX0ZFx3dsfvs17e7gm91fApy+Fhw9LCp2Q6rpG6EvPkiVdy37xi8S/eEMDrF/v1F0+ea8GybptGyxf7txPP/qo+zljxsDPflZ4L4aZmG6OBt5U1bdUtRVYDpzmb6Cqq1TVs2q9gEu9Zhj9StiE16jho2htb82yNPlJmFmnsrLrFMell/6ly3mHHQb77OOSoaTybs32j0DYPYMmVL/1LRg3Ds4+G3bsCL7epk2pn1G+kkkIhFLAb7XaDByTov0s4Cnf8QgRWQO0AQtU9fHkE0RkNjAbYPz48aGZzaOWBR6iJ1PU5IH+k+n2T95OQ1MDHepmE3e272TFuyv48/Y/8+k7P828Q+Yxumg0jR820treyrDiYZSOKmXsyLFprtwdEZkG3AUUA4tVdUFS/XDgQaACeA84S1Xr+/YNs0NYaAV/ILXychdjoaQEbr7ZhV247DLnNOWnpQXmzoXXX4dbb02MkpODrNXWpkkkn6Leq5s71/3QeHWeMvcmThsa4IIL4Lnn4KGHuk+odnS4GEG/+52zxQc5Y3nz9qnyIeclYTYdbwO+huvo3vE3gbtD2p6LG9EP95WVxj//DagHPpHqflGwY/aEqMkUNXlU+9m2um6Zxu6IqcwXjd0R02Xrlunjrz6u+9yyjw69YagOvWFop28z89GS6hJdtq6rnZ40Nnqccv97vM8OA14CJia1uRi4N74/HXgk1TU1T/q2N8WxcOGqblMcqfzzw7bx41XvvLO7PbykJHHtZcvccVC9v27hwlUK7lo33qi6zz49l0ck/T0zJSp/M49U/TqTEX0j4A8reEC8rAsiciJQBRynqrt8PySN8c+3RKQOOCL+T2QYPSYsKNRnJ3yW8jvL+aitq+HVi3PfQ6+ITnMlgIh45soNvjanAfPj+48Bd4s4NdKTG0UNbyRbV+emOPyUlQUHTSstDU9m/q9/BS9xaGlxdvLnnw8efbe0wJw5bhTu1d1661GAe2v44Q9Tf4+ysvQjdkj9llFIZKLoVwMHiciBOAU/HTjH30BEjgDuA6ap6hZf+RigRVV3icg44PN0nag1jH5h3z327abkPXoR5z4Tc2VnG1VtE5Em4GPAVn+jfDVLBslz++1O0fv98D3XxMZGaPVNk7S1CR98MJwPPxzJli0j2LbNbU1Nw2ltLaK1tZjW1mIeeqiYlpZi/NOFxcUdDB3aQVFRO8OHdzB6dDtDh3aw774tHHzw+4wZs5MxY3ay7747GTVqJyNHtiGSuPewYe7HJ0xW72uVlsLSpV2/d0/+BFH7m6UiraKPd+JLgadxr7RLVPUVEbkB96qwArgV2BP4pbgnvlGdh82ngPtEpAP3l1ygqhsCb2QYfSQ2OhYYKjaXqxZVtQaoAZgyZYpOnTo1sF1dXR1hdbkgTJ4gW/qZZ3a3l4Oz79fUuPZBbwKxmHtrKC/vWt/eXkR7exHjxzv15NVddtlf+N73pnaeG5ScxbunJ1OQrP1F1P5mqcgoHr2qPgk8mVR2rW//xJDzngMm9UVAw8iUoDj3vVy1mIm50muzWUSGAKNxk7IFTdgkZTpTSJBC9twVwxS2Vx9Wl+6eBTeh2gcs8YhRMPgTkm9s2kjZ6DKqT6juzarFtOZKYAUwA3ge57Dwh3y3z/eV3v4IZGIv90I0eCN5U+Y9wxS9UVD0RwafDM2V9wMPicibwDbcj4ERQjqFnKo+1QSxkRkStUGIiLwLhCVEG0fSZFcEiJpMUZMHoidTTFX3yfZN86xvR00eiJ5MUZMntF9HTtGnQkTWqOqUXMvhJ2oyRU0eiKZMUSNqzyhq8kD0ZIqaPKmw6JWGYRgFjil6wzCMAiffFH1NrgUIIGoyRU0eiKZMUSNqzyhq8kD0ZIqaPKHklY3eMAzD6Dn5NqI3DMMweogpesMwjAInbxS9iEwTkddE5E0RuSoC8tSLyHoReTEebz8XMiwRkS0i8rKvbKyIPCMib8Q/x+RYnvki0hh/Ti+KyJezJU8+ELV+Dda3eyBP3vTtvFD0GaYzzAX/qS5FYq58aZfi8vn6uQpYqaoH4fL3ZlN5BMkDcIcm0kk+GVA/KIlwvwbr25nIA3nSt/NC0ZNBOsPBiKo+i1t+7+c04IH4/gPA6TmWxwjH+nUI1rf7l3xR9EHxwUtzJIuHAr8XkbXxmONRYbyqvh3ffwcYn0th4lwqIuvir79Ze93OA6LYr8H6dk/Ii76dL4o+ivyHqh6Je+2+RESOzbVAycSjKebaf/ZnwCeAycDbwG05lcbIBOvbmZE3fTtfFH1G6QyziS9F4hbg17jX8CjwLxHZDyD+uSVN+wFFVf+lqu2q2gEsIjrPKQpErl+D9e1Myae+nS+KvjM+uIgMw4WEXZErYURkDxEZ5e0DJwEvpz4ra3hx0ol//iaHsnj/kB5nEJ3nFAUi1a/B+nZPyKe+nRfx6MPig+dQpPHAr+NpE4cAv1DV32VbCBF5GJgKjBORzcB1wALgURGZhQuJ+40cyzNVRCbjXrPrgYuyJU/UiWC/BuvbPZEnb/q2hUAwDMMocPLFdGMYhmH0ElP0hmEYBY4pesMwjALHFL1hGEaBY4reMAyjwDFFbxiGUeCYojcMwyhw/n9uIJ7YToa9lwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "subplots = [221, 222, 223, 224]\n",
    "plt.figure(1)\n",
    "for plot_id, i in enumerate(np.random.choice(range(len(x_test)), size=4, replace=False)):\n",
    "    p1 = np.expand_dims(predicts[i][:,0], axis=-1)\n",
    "    x1 = np.expand_dims(x_test[i][:,0], axis=-1)\n",
    "    y1 = np.expand_dims(y_test[i][:,0], axis=-1)\n",
    "    ff, xx, yy = p1 * norm_constant, x1 * norm_constant, y1 * norm_constant\n",
    "    plt.subplot(subplots[plot_id])\n",
    "    plt.grid()\n",
    "    plot_scatter(range(0, backcast_length), xx, color='b')\n",
    "    plot_scatter(range(backcast_length, backcast_length + forecast_length), yy, color='g')\n",
    "    plot_scatter(range(backcast_length, backcast_length + forecast_length), ff, color='r')\n",
    "plt.savefig(\"nbeats-REAL-predictions-sines.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d8703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
